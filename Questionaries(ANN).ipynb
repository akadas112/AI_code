{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mQBFK3tYkNO",
        "outputId": "29e20cb9-755a-4713-deaf-a0f0623bfdfa"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRgXkVVAW9K4"
      },
      "source": [
        "#Importing important files....\n",
        "import io\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn import tree\n",
        "from sklearn.svm import SVC\n",
        "from google.colab import files\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import linear_model\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBClassifier\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn import metrics\n",
        "from xgboost import XGBRegressor, plot_tree\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.inspection import permutation_importance\n",
        "from statistics import mean\n",
        "from sklearn import tree\n",
        "from datetime import datetime\n",
        "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import accuracy_score\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VasglcHqYpWV"
      },
      "source": [
        "\n",
        "df = pd.read_excel('/content/drive/MyDrive/Parkinson all docs/Dataset/Springer(dataset).xlsx')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "UGUIuEsbYpTl",
        "outputId": "da1976d8-8d1b-4eff-e58a-4749f17d333a"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PAT_NO</th>\n",
              "      <th>Event_ID</th>\n",
              "      <th>NP1SLPN</th>\n",
              "      <th>NP1SLPD</th>\n",
              "      <th>NP1PAIN</th>\n",
              "      <th>NP1URIN</th>\n",
              "      <th>NP1CNST</th>\n",
              "      <th>NP1LTHD</th>\n",
              "      <th>NP1FATG</th>\n",
              "      <th>NP2SPCH</th>\n",
              "      <th>NP2SALV</th>\n",
              "      <th>NP2SWAL</th>\n",
              "      <th>NP2EAT</th>\n",
              "      <th>NP2DRES</th>\n",
              "      <th>NP2HYGN</th>\n",
              "      <th>NP2HWRT</th>\n",
              "      <th>NP2HOBB</th>\n",
              "      <th>NP2TURN</th>\n",
              "      <th>NP2TRMR</th>\n",
              "      <th>NP2RISE</th>\n",
              "      <th>NP2WALK</th>\n",
              "      <th>NP2FREZ</th>\n",
              "      <th>NP3SPCH</th>\n",
              "      <th>NP3FACXP</th>\n",
              "      <th>NP3RIGN</th>\n",
              "      <th>NP3RIGRU</th>\n",
              "      <th>NP3RIGLU</th>\n",
              "      <th>PN3RIGRL</th>\n",
              "      <th>NP3RIGLL</th>\n",
              "      <th>NP3FTAPR</th>\n",
              "      <th>NP3FTAPL</th>\n",
              "      <th>NP3HMOVR</th>\n",
              "      <th>NP3HMOVL</th>\n",
              "      <th>NP3PRSPR</th>\n",
              "      <th>NP3PRSPL</th>\n",
              "      <th>NP3TTAPR</th>\n",
              "      <th>NP3TTAPL</th>\n",
              "      <th>NP3LGAGR</th>\n",
              "      <th>NP3LGAGL</th>\n",
              "      <th>NP3RISNG</th>\n",
              "      <th>NP3GAIT</th>\n",
              "      <th>NP3FRZGT</th>\n",
              "      <th>NP3PSTBL</th>\n",
              "      <th>NP3POSTR</th>\n",
              "      <th>NP3BRADY</th>\n",
              "      <th>NP3PTRMR</th>\n",
              "      <th>NP3PTRML</th>\n",
              "      <th>NP3KTRMR</th>\n",
              "      <th>NP3KTRML</th>\n",
              "      <th>NP3RTARU</th>\n",
              "      <th>NP3RTALU</th>\n",
              "      <th>NP3RTARL</th>\n",
              "      <th>NP3RTALL</th>\n",
              "      <th>NP3RTALJ</th>\n",
              "      <th>NP3RTCON</th>\n",
              "      <th>Status</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3000</td>\n",
              "      <td>BL</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3000</td>\n",
              "      <td>V04</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3000</td>\n",
              "      <td>V06</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3000</td>\n",
              "      <td>V08</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3000</td>\n",
              "      <td>V10</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5881</th>\n",
              "      <td>3952</td>\n",
              "      <td>V04</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5882</th>\n",
              "      <td>3952</td>\n",
              "      <td>V06</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5883</th>\n",
              "      <td>3952</td>\n",
              "      <td>V08</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5884</th>\n",
              "      <td>3952</td>\n",
              "      <td>V10</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5885</th>\n",
              "      <td>3952</td>\n",
              "      <td>V12</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5886 rows Ã— 56 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      PAT_NO Event_ID  NP1SLPN  NP1SLPD  ...  NP3RTALL  NP3RTALJ  NP3RTCON  Status\n",
              "0       3000       BL        1        2  ...       0.0       0.0       0.0       0\n",
              "1       3000      V04        0        2  ...       0.0       0.0       0.0       0\n",
              "2       3000      V06        2        1  ...       0.0       0.0       0.0       0\n",
              "3       3000      V08        3        2  ...       0.0       0.0       0.0       0\n",
              "4       3000      V10        1        1  ...       0.0       0.0       0.0       0\n",
              "...      ...      ...      ...      ...  ...       ...       ...       ...     ...\n",
              "5881    3952      V04        2        0  ...       0.0       0.0       0.0       0\n",
              "5882    3952      V06        2        2  ...       0.0       0.0       0.0       0\n",
              "5883    3952      V08        2        2  ...       0.0       0.0       0.0       0\n",
              "5884    3952      V10        2        1  ...       0.0       0.0       0.0       0\n",
              "5885    3952      V12        2        1  ...       0.0       0.0       0.0       0\n",
              "\n",
              "[5886 rows x 56 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 154
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iY25CNkaYpQ9"
      },
      "source": [
        "_#Defining features(X) and labels(Y)....\n",
        "X_train = df.drop(['Status','PAT_NO','Event_ID'],axis =1).values\n",
        "y_train = df['Status'].values\n",
        "X_test = df_test.drop(['Status','PAT_NO','Event_ID'],axis =1).values\n",
        "y_test = df_test['Status'].values\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZqGKt9XXYpPJ",
        "outputId": "a17d72a2-ddab-43ff-a10b-dd2c7bd2fd69"
      },
      "source": [
        "#Checking is any nan value available or not\n",
        "np.any(np.isnan(X_train))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHy1GDlxYpMl"
      },
      "source": [
        "#Missing value handle\n",
        "imputer = SimpleImputer(missing_values = np.NaN, strategy = 'mean')\n",
        "imputer = imputer.fit(X_train)\n",
        "X_train = imputer.transform(X_train)\n",
        "imputer1 = SimpleImputer(missing_values = np.NaN, strategy = 'mean')\n",
        "imputer1 = imputer1.fit(X_test)\n",
        "X_test = imputer.transform(X_test)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0K26r1SqYpJ9",
        "outputId": "daff8385-a6e0-40b0-ab33-df3a734132ba"
      },
      "source": [
        "#Checking is any nan value available or not\n",
        "np.any(np.isnan(y_train))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 158
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Urjj-FCwYpHd"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "X_train = StandardScaler().fit_transform(X_train)\n",
        "X_test = StandardScaler().fit_transform(X_test)\n",
        "\n",
        "feature_X_train = pd.DataFrame(X_train)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "Grw5JR8vY2Ys",
        "outputId": "5a60ede2-78e4-4c65-e8cc-041aa2afa87a"
      },
      "source": [
        "feature_X_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "      <th>51</th>\n",
              "      <th>52</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.033938</td>\n",
              "      <td>1.223181</td>\n",
              "      <td>-0.875557</td>\n",
              "      <td>-0.844657</td>\n",
              "      <td>-0.703432</td>\n",
              "      <td>-0.556254</td>\n",
              "      <td>-0.866701</td>\n",
              "      <td>-0.682608</td>\n",
              "      <td>-0.645262</td>\n",
              "      <td>-0.433031</td>\n",
              "      <td>-0.669385</td>\n",
              "      <td>-0.779283</td>\n",
              "      <td>-0.616358</td>\n",
              "      <td>-0.914815</td>\n",
              "      <td>-0.724629</td>\n",
              "      <td>-0.663363</td>\n",
              "      <td>-1.178439</td>\n",
              "      <td>-0.787041</td>\n",
              "      <td>-0.747470</td>\n",
              "      <td>-0.305947</td>\n",
              "      <td>-0.868624</td>\n",
              "      <td>-1.256402</td>\n",
              "      <td>-0.791276</td>\n",
              "      <td>-1.117881</td>\n",
              "      <td>-0.939107</td>\n",
              "      <td>-0.756551</td>\n",
              "      <td>-0.698727</td>\n",
              "      <td>-1.041878</td>\n",
              "      <td>-0.998286</td>\n",
              "      <td>-0.906990</td>\n",
              "      <td>-0.896863</td>\n",
              "      <td>-0.911186</td>\n",
              "      <td>0.166604</td>\n",
              "      <td>-0.947406</td>\n",
              "      <td>0.041445</td>\n",
              "      <td>-0.743906</td>\n",
              "      <td>0.419833</td>\n",
              "      <td>-0.45214</td>\n",
              "      <td>-0.967793</td>\n",
              "      <td>-0.166494</td>\n",
              "      <td>-0.344881</td>\n",
              "      <td>0.258380</td>\n",
              "      <td>-1.274522</td>\n",
              "      <td>-0.604618</td>\n",
              "      <td>-0.590244</td>\n",
              "      <td>-0.579538</td>\n",
              "      <td>-0.635316</td>\n",
              "      <td>-0.640792</td>\n",
              "      <td>-0.592144</td>\n",
              "      <td>-0.319334</td>\n",
              "      <td>-0.307662</td>\n",
              "      <td>-0.252019</td>\n",
              "      <td>-0.897763</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.950271</td>\n",
              "      <td>1.223181</td>\n",
              "      <td>-0.875557</td>\n",
              "      <td>0.257460</td>\n",
              "      <td>-0.703432</td>\n",
              "      <td>-0.556254</td>\n",
              "      <td>0.281229</td>\n",
              "      <td>-0.682608</td>\n",
              "      <td>-0.645262</td>\n",
              "      <td>-0.433031</td>\n",
              "      <td>-0.669385</td>\n",
              "      <td>-0.779283</td>\n",
              "      <td>-0.616358</td>\n",
              "      <td>-0.914815</td>\n",
              "      <td>-0.724629</td>\n",
              "      <td>-0.663363</td>\n",
              "      <td>-1.178439</td>\n",
              "      <td>-0.787041</td>\n",
              "      <td>-0.747470</td>\n",
              "      <td>-0.305947</td>\n",
              "      <td>-0.868624</td>\n",
              "      <td>-1.256402</td>\n",
              "      <td>-0.791276</td>\n",
              "      <td>-1.117881</td>\n",
              "      <td>-0.939107</td>\n",
              "      <td>-0.756551</td>\n",
              "      <td>-0.698727</td>\n",
              "      <td>-1.041878</td>\n",
              "      <td>-0.998286</td>\n",
              "      <td>-0.906990</td>\n",
              "      <td>-0.896863</td>\n",
              "      <td>-0.911186</td>\n",
              "      <td>-0.881667</td>\n",
              "      <td>-0.947406</td>\n",
              "      <td>-0.991687</td>\n",
              "      <td>-0.743906</td>\n",
              "      <td>-0.771246</td>\n",
              "      <td>-0.45214</td>\n",
              "      <td>-0.967793</td>\n",
              "      <td>-0.166494</td>\n",
              "      <td>-0.344881</td>\n",
              "      <td>0.258380</td>\n",
              "      <td>-1.274522</td>\n",
              "      <td>-0.604618</td>\n",
              "      <td>-0.590244</td>\n",
              "      <td>-0.579538</td>\n",
              "      <td>-0.635316</td>\n",
              "      <td>-0.640792</td>\n",
              "      <td>-0.592144</td>\n",
              "      <td>-0.319334</td>\n",
              "      <td>-0.307662</td>\n",
              "      <td>-0.252019</td>\n",
              "      <td>-0.897763</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.882395</td>\n",
              "      <td>0.070500</td>\n",
              "      <td>-0.875557</td>\n",
              "      <td>-0.844657</td>\n",
              "      <td>-0.703432</td>\n",
              "      <td>-0.556254</td>\n",
              "      <td>-0.866701</td>\n",
              "      <td>-0.682608</td>\n",
              "      <td>-0.645262</td>\n",
              "      <td>-0.433031</td>\n",
              "      <td>-0.669385</td>\n",
              "      <td>-0.779283</td>\n",
              "      <td>-0.616358</td>\n",
              "      <td>-0.914815</td>\n",
              "      <td>-0.724629</td>\n",
              "      <td>-0.663363</td>\n",
              "      <td>-1.178439</td>\n",
              "      <td>-0.787041</td>\n",
              "      <td>-0.747470</td>\n",
              "      <td>-0.305947</td>\n",
              "      <td>-0.868624</td>\n",
              "      <td>-1.256402</td>\n",
              "      <td>-0.791276</td>\n",
              "      <td>-1.117881</td>\n",
              "      <td>-0.939107</td>\n",
              "      <td>-0.756551</td>\n",
              "      <td>-0.698727</td>\n",
              "      <td>0.025575</td>\n",
              "      <td>0.020437</td>\n",
              "      <td>-0.906990</td>\n",
              "      <td>-0.896863</td>\n",
              "      <td>-0.911186</td>\n",
              "      <td>0.166604</td>\n",
              "      <td>-0.947406</td>\n",
              "      <td>-0.991687</td>\n",
              "      <td>-0.743906</td>\n",
              "      <td>-0.771246</td>\n",
              "      <td>-0.45214</td>\n",
              "      <td>-0.967793</td>\n",
              "      <td>-0.166494</td>\n",
              "      <td>-0.344881</td>\n",
              "      <td>0.258380</td>\n",
              "      <td>-1.274522</td>\n",
              "      <td>-0.604618</td>\n",
              "      <td>-0.590244</td>\n",
              "      <td>-0.579538</td>\n",
              "      <td>-0.635316</td>\n",
              "      <td>-0.640792</td>\n",
              "      <td>-0.592144</td>\n",
              "      <td>-0.319334</td>\n",
              "      <td>-0.307662</td>\n",
              "      <td>-0.252019</td>\n",
              "      <td>-0.897763</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.798728</td>\n",
              "      <td>1.223181</td>\n",
              "      <td>-0.875557</td>\n",
              "      <td>0.257460</td>\n",
              "      <td>-0.703432</td>\n",
              "      <td>-0.556254</td>\n",
              "      <td>-0.866701</td>\n",
              "      <td>-0.682608</td>\n",
              "      <td>-0.645262</td>\n",
              "      <td>-0.433031</td>\n",
              "      <td>-0.669385</td>\n",
              "      <td>-0.779283</td>\n",
              "      <td>-0.616358</td>\n",
              "      <td>-0.914815</td>\n",
              "      <td>-0.724629</td>\n",
              "      <td>-0.663363</td>\n",
              "      <td>-1.178439</td>\n",
              "      <td>-0.787041</td>\n",
              "      <td>-0.747470</td>\n",
              "      <td>-0.305947</td>\n",
              "      <td>-0.868624</td>\n",
              "      <td>-1.256402</td>\n",
              "      <td>-0.791276</td>\n",
              "      <td>-1.117881</td>\n",
              "      <td>-0.939107</td>\n",
              "      <td>-0.756551</td>\n",
              "      <td>-0.698727</td>\n",
              "      <td>-1.041878</td>\n",
              "      <td>-0.998286</td>\n",
              "      <td>0.235728</td>\n",
              "      <td>-0.896863</td>\n",
              "      <td>-0.911186</td>\n",
              "      <td>-0.881667</td>\n",
              "      <td>-0.947406</td>\n",
              "      <td>-0.991687</td>\n",
              "      <td>-0.743906</td>\n",
              "      <td>-0.771246</td>\n",
              "      <td>-0.45214</td>\n",
              "      <td>-0.967793</td>\n",
              "      <td>-0.166494</td>\n",
              "      <td>-0.344881</td>\n",
              "      <td>0.258380</td>\n",
              "      <td>-1.274522</td>\n",
              "      <td>-0.604618</td>\n",
              "      <td>-0.590244</td>\n",
              "      <td>-0.579538</td>\n",
              "      <td>-0.635316</td>\n",
              "      <td>-0.640792</td>\n",
              "      <td>-0.592144</td>\n",
              "      <td>-0.319334</td>\n",
              "      <td>-0.307662</td>\n",
              "      <td>-0.252019</td>\n",
              "      <td>-0.897763</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.033938</td>\n",
              "      <td>0.070500</td>\n",
              "      <td>0.230350</td>\n",
              "      <td>0.257460</td>\n",
              "      <td>0.595721</td>\n",
              "      <td>-0.556254</td>\n",
              "      <td>0.281229</td>\n",
              "      <td>-0.682608</td>\n",
              "      <td>-0.645262</td>\n",
              "      <td>-0.433031</td>\n",
              "      <td>-0.669385</td>\n",
              "      <td>-0.779283</td>\n",
              "      <td>-0.616358</td>\n",
              "      <td>-0.914815</td>\n",
              "      <td>-0.724629</td>\n",
              "      <td>-0.663363</td>\n",
              "      <td>-1.178439</td>\n",
              "      <td>-0.787041</td>\n",
              "      <td>-0.747470</td>\n",
              "      <td>-0.305947</td>\n",
              "      <td>-0.868624</td>\n",
              "      <td>-1.256402</td>\n",
              "      <td>-0.791276</td>\n",
              "      <td>-1.117881</td>\n",
              "      <td>-0.939107</td>\n",
              "      <td>-0.756551</td>\n",
              "      <td>-0.698727</td>\n",
              "      <td>0.025575</td>\n",
              "      <td>0.020437</td>\n",
              "      <td>1.378446</td>\n",
              "      <td>1.269681</td>\n",
              "      <td>0.213700</td>\n",
              "      <td>0.166604</td>\n",
              "      <td>0.162240</td>\n",
              "      <td>1.074577</td>\n",
              "      <td>0.585143</td>\n",
              "      <td>1.610913</td>\n",
              "      <td>-0.45214</td>\n",
              "      <td>-0.967793</td>\n",
              "      <td>-0.166494</td>\n",
              "      <td>-0.344881</td>\n",
              "      <td>0.258380</td>\n",
              "      <td>-1.274522</td>\n",
              "      <td>0.888532</td>\n",
              "      <td>1.077862</td>\n",
              "      <td>1.210793</td>\n",
              "      <td>1.059532</td>\n",
              "      <td>-0.640792</td>\n",
              "      <td>-0.592144</td>\n",
              "      <td>-0.319334</td>\n",
              "      <td>-0.307662</td>\n",
              "      <td>-0.252019</td>\n",
              "      <td>-0.897763</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5881</th>\n",
              "      <td>0.882395</td>\n",
              "      <td>-1.082181</td>\n",
              "      <td>-0.875557</td>\n",
              "      <td>-0.844657</td>\n",
              "      <td>0.595721</td>\n",
              "      <td>-0.556254</td>\n",
              "      <td>-0.866701</td>\n",
              "      <td>-0.682608</td>\n",
              "      <td>-0.645262</td>\n",
              "      <td>-0.433031</td>\n",
              "      <td>-0.669385</td>\n",
              "      <td>-0.779283</td>\n",
              "      <td>-0.616358</td>\n",
              "      <td>-0.914815</td>\n",
              "      <td>-0.724629</td>\n",
              "      <td>-0.663363</td>\n",
              "      <td>-1.178439</td>\n",
              "      <td>-0.787041</td>\n",
              "      <td>-0.747470</td>\n",
              "      <td>-0.305947</td>\n",
              "      <td>-0.868624</td>\n",
              "      <td>-1.256402</td>\n",
              "      <td>-0.791276</td>\n",
              "      <td>-1.117881</td>\n",
              "      <td>-0.939107</td>\n",
              "      <td>-0.756551</td>\n",
              "      <td>-0.698727</td>\n",
              "      <td>-1.041878</td>\n",
              "      <td>-0.998286</td>\n",
              "      <td>-0.906990</td>\n",
              "      <td>-0.896863</td>\n",
              "      <td>-0.911186</td>\n",
              "      <td>-0.881667</td>\n",
              "      <td>-0.947406</td>\n",
              "      <td>-0.991687</td>\n",
              "      <td>-0.743906</td>\n",
              "      <td>-0.771246</td>\n",
              "      <td>-0.45214</td>\n",
              "      <td>-0.967793</td>\n",
              "      <td>-0.166494</td>\n",
              "      <td>-0.344881</td>\n",
              "      <td>-0.952264</td>\n",
              "      <td>-1.274522</td>\n",
              "      <td>-0.604618</td>\n",
              "      <td>-0.590244</td>\n",
              "      <td>-0.579538</td>\n",
              "      <td>-0.635316</td>\n",
              "      <td>-0.640792</td>\n",
              "      <td>-0.592144</td>\n",
              "      <td>-0.319334</td>\n",
              "      <td>-0.307662</td>\n",
              "      <td>-0.252019</td>\n",
              "      <td>-0.897763</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5882</th>\n",
              "      <td>0.882395</td>\n",
              "      <td>1.223181</td>\n",
              "      <td>1.336257</td>\n",
              "      <td>-0.844657</td>\n",
              "      <td>0.595721</td>\n",
              "      <td>-0.556254</td>\n",
              "      <td>0.281229</td>\n",
              "      <td>-0.682608</td>\n",
              "      <td>-0.645262</td>\n",
              "      <td>-0.433031</td>\n",
              "      <td>-0.669385</td>\n",
              "      <td>-0.779283</td>\n",
              "      <td>-0.616358</td>\n",
              "      <td>-0.914815</td>\n",
              "      <td>-0.724629</td>\n",
              "      <td>-0.663363</td>\n",
              "      <td>-1.178439</td>\n",
              "      <td>-0.787041</td>\n",
              "      <td>-0.747470</td>\n",
              "      <td>-0.305947</td>\n",
              "      <td>-0.868624</td>\n",
              "      <td>-1.256402</td>\n",
              "      <td>-0.791276</td>\n",
              "      <td>-1.117881</td>\n",
              "      <td>-0.939107</td>\n",
              "      <td>-0.756551</td>\n",
              "      <td>-0.698727</td>\n",
              "      <td>-1.041878</td>\n",
              "      <td>-0.998286</td>\n",
              "      <td>-0.906990</td>\n",
              "      <td>-0.896863</td>\n",
              "      <td>-0.911186</td>\n",
              "      <td>-0.881667</td>\n",
              "      <td>-0.947406</td>\n",
              "      <td>-0.991687</td>\n",
              "      <td>-0.743906</td>\n",
              "      <td>-0.771246</td>\n",
              "      <td>-0.45214</td>\n",
              "      <td>-0.967793</td>\n",
              "      <td>-0.166494</td>\n",
              "      <td>-0.344881</td>\n",
              "      <td>-0.952264</td>\n",
              "      <td>-1.274522</td>\n",
              "      <td>-0.604618</td>\n",
              "      <td>-0.590244</td>\n",
              "      <td>-0.579538</td>\n",
              "      <td>-0.635316</td>\n",
              "      <td>-0.640792</td>\n",
              "      <td>-0.592144</td>\n",
              "      <td>-0.319334</td>\n",
              "      <td>-0.307662</td>\n",
              "      <td>-0.252019</td>\n",
              "      <td>-0.897763</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5883</th>\n",
              "      <td>0.882395</td>\n",
              "      <td>1.223181</td>\n",
              "      <td>0.230350</td>\n",
              "      <td>0.257460</td>\n",
              "      <td>0.595721</td>\n",
              "      <td>-0.556254</td>\n",
              "      <td>0.281229</td>\n",
              "      <td>-0.682608</td>\n",
              "      <td>-0.645262</td>\n",
              "      <td>-0.433031</td>\n",
              "      <td>-0.669385</td>\n",
              "      <td>-0.779283</td>\n",
              "      <td>-0.616358</td>\n",
              "      <td>-0.914815</td>\n",
              "      <td>-0.724629</td>\n",
              "      <td>-0.663363</td>\n",
              "      <td>-1.178439</td>\n",
              "      <td>-0.787041</td>\n",
              "      <td>-0.747470</td>\n",
              "      <td>-0.305947</td>\n",
              "      <td>-0.868624</td>\n",
              "      <td>-1.256402</td>\n",
              "      <td>-0.791276</td>\n",
              "      <td>-1.117881</td>\n",
              "      <td>-0.939107</td>\n",
              "      <td>-0.756551</td>\n",
              "      <td>-0.698727</td>\n",
              "      <td>-1.041878</td>\n",
              "      <td>-0.998286</td>\n",
              "      <td>-0.906990</td>\n",
              "      <td>-0.896863</td>\n",
              "      <td>-0.911186</td>\n",
              "      <td>-0.881667</td>\n",
              "      <td>-0.947406</td>\n",
              "      <td>-0.991687</td>\n",
              "      <td>-0.743906</td>\n",
              "      <td>-0.771246</td>\n",
              "      <td>-0.45214</td>\n",
              "      <td>-0.967793</td>\n",
              "      <td>-0.166494</td>\n",
              "      <td>-0.344881</td>\n",
              "      <td>-0.952264</td>\n",
              "      <td>-1.274522</td>\n",
              "      <td>-0.604618</td>\n",
              "      <td>-0.590244</td>\n",
              "      <td>-0.579538</td>\n",
              "      <td>-0.635316</td>\n",
              "      <td>-0.640792</td>\n",
              "      <td>-0.592144</td>\n",
              "      <td>-0.319334</td>\n",
              "      <td>-0.307662</td>\n",
              "      <td>-0.252019</td>\n",
              "      <td>-0.897763</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5884</th>\n",
              "      <td>0.882395</td>\n",
              "      <td>0.070500</td>\n",
              "      <td>0.230350</td>\n",
              "      <td>-0.844657</td>\n",
              "      <td>0.595721</td>\n",
              "      <td>-0.556254</td>\n",
              "      <td>0.281229</td>\n",
              "      <td>-0.682608</td>\n",
              "      <td>-0.645262</td>\n",
              "      <td>-0.433031</td>\n",
              "      <td>-0.669385</td>\n",
              "      <td>-0.779283</td>\n",
              "      <td>-0.616358</td>\n",
              "      <td>-0.914815</td>\n",
              "      <td>0.526249</td>\n",
              "      <td>-0.663363</td>\n",
              "      <td>-1.178439</td>\n",
              "      <td>-0.787041</td>\n",
              "      <td>0.608341</td>\n",
              "      <td>-0.305947</td>\n",
              "      <td>-0.868624</td>\n",
              "      <td>-1.256402</td>\n",
              "      <td>-0.791276</td>\n",
              "      <td>-1.117881</td>\n",
              "      <td>-0.939107</td>\n",
              "      <td>-0.756551</td>\n",
              "      <td>-0.698727</td>\n",
              "      <td>-1.041878</td>\n",
              "      <td>-0.998286</td>\n",
              "      <td>-0.906990</td>\n",
              "      <td>-0.896863</td>\n",
              "      <td>-0.911186</td>\n",
              "      <td>-0.881667</td>\n",
              "      <td>-0.947406</td>\n",
              "      <td>-0.991687</td>\n",
              "      <td>-0.743906</td>\n",
              "      <td>-0.771246</td>\n",
              "      <td>-0.45214</td>\n",
              "      <td>-0.967793</td>\n",
              "      <td>-0.166494</td>\n",
              "      <td>-0.344881</td>\n",
              "      <td>-0.952264</td>\n",
              "      <td>-1.274522</td>\n",
              "      <td>-0.604618</td>\n",
              "      <td>-0.590244</td>\n",
              "      <td>-0.579538</td>\n",
              "      <td>-0.635316</td>\n",
              "      <td>-0.640792</td>\n",
              "      <td>-0.592144</td>\n",
              "      <td>-0.319334</td>\n",
              "      <td>-0.307662</td>\n",
              "      <td>-0.252019</td>\n",
              "      <td>-0.897763</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5885</th>\n",
              "      <td>0.882395</td>\n",
              "      <td>0.070500</td>\n",
              "      <td>0.230350</td>\n",
              "      <td>-0.844657</td>\n",
              "      <td>0.595721</td>\n",
              "      <td>-0.556254</td>\n",
              "      <td>-0.866701</td>\n",
              "      <td>-0.682608</td>\n",
              "      <td>-0.645262</td>\n",
              "      <td>-0.433031</td>\n",
              "      <td>-0.669385</td>\n",
              "      <td>-0.779283</td>\n",
              "      <td>-0.616358</td>\n",
              "      <td>-0.914815</td>\n",
              "      <td>-0.724629</td>\n",
              "      <td>-0.663363</td>\n",
              "      <td>-1.178439</td>\n",
              "      <td>-0.787041</td>\n",
              "      <td>-0.747470</td>\n",
              "      <td>-0.305947</td>\n",
              "      <td>-0.868624</td>\n",
              "      <td>-1.256402</td>\n",
              "      <td>-0.791276</td>\n",
              "      <td>-1.117881</td>\n",
              "      <td>-0.939107</td>\n",
              "      <td>-0.756551</td>\n",
              "      <td>-0.698727</td>\n",
              "      <td>-1.041878</td>\n",
              "      <td>-0.998286</td>\n",
              "      <td>-0.906990</td>\n",
              "      <td>-0.896863</td>\n",
              "      <td>-0.911186</td>\n",
              "      <td>-0.881667</td>\n",
              "      <td>-0.947406</td>\n",
              "      <td>-0.991687</td>\n",
              "      <td>-0.743906</td>\n",
              "      <td>-0.771246</td>\n",
              "      <td>-0.45214</td>\n",
              "      <td>-0.967793</td>\n",
              "      <td>-0.166494</td>\n",
              "      <td>-0.344881</td>\n",
              "      <td>-0.952264</td>\n",
              "      <td>-1.274522</td>\n",
              "      <td>-0.604618</td>\n",
              "      <td>-0.590244</td>\n",
              "      <td>-0.579538</td>\n",
              "      <td>-0.635316</td>\n",
              "      <td>-0.640792</td>\n",
              "      <td>-0.592144</td>\n",
              "      <td>-0.319334</td>\n",
              "      <td>-0.307662</td>\n",
              "      <td>-0.252019</td>\n",
              "      <td>-0.897763</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5886 rows Ã— 53 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            0         1         2   ...        50        51        52\n",
              "0    -0.033938  1.223181 -0.875557  ... -0.307662 -0.252019 -0.897763\n",
              "1    -0.950271  1.223181 -0.875557  ... -0.307662 -0.252019 -0.897763\n",
              "2     0.882395  0.070500 -0.875557  ... -0.307662 -0.252019 -0.897763\n",
              "3     1.798728  1.223181 -0.875557  ... -0.307662 -0.252019 -0.897763\n",
              "4    -0.033938  0.070500  0.230350  ... -0.307662 -0.252019 -0.897763\n",
              "...        ...       ...       ...  ...       ...       ...       ...\n",
              "5881  0.882395 -1.082181 -0.875557  ... -0.307662 -0.252019 -0.897763\n",
              "5882  0.882395  1.223181  1.336257  ... -0.307662 -0.252019 -0.897763\n",
              "5883  0.882395  1.223181  0.230350  ... -0.307662 -0.252019 -0.897763\n",
              "5884  0.882395  0.070500  0.230350  ... -0.307662 -0.252019 -0.897763\n",
              "5885  0.882395  0.070500  0.230350  ... -0.307662 -0.252019 -0.897763\n",
              "\n",
              "[5886 rows x 53 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 160
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_GJv-WOY5IM",
        "outputId": "351ed305-730e-419e-fa5b-86757f6a6b88"
      },
      "source": [
        "print(\"Before OverSampling, counts of label '1': {}\".format(sum(y_train == 1)))\n",
        "print(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y_train == 0)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before OverSampling, counts of label '1': 4763\n",
            "Before OverSampling, counts of label '0': 1123 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPt_9y29Y5Bs",
        "outputId": "e87b3c9d-c8c1-4d22-ddaa-ec340ee44c44"
      },
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "sm = SMOTE(random_state = 2)\n",
        "X_res, y_res = sm.fit_sample(X_train, y_train.ravel())\n",
        "\n",
        "print('After OverSampling, the shape of train_X: {}'.format(X_res.shape))\n",
        "print('After OverSampling, the shape of train_y: {} \\n'.format(y_res.shape))\n",
        "\n",
        "print(\"After OverSampling, counts of label '1': {}\".format(sum(y_res == 1)))\n",
        "print(\"After OverSampling, counts of label '0': {}\".format(sum(y_res == 0)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After OverSampling, the shape of train_X: (9526, 53)\n",
            "After OverSampling, the shape of train_y: (9526,) \n",
            "\n",
            "After OverSampling, counts of label '1': 4763\n",
            "After OverSampling, counts of label '0': 4763\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-PUsA7oY4_c",
        "outputId": "968bedc5-3391-4668-d705-41194a869b08"
      },
      "source": [
        "#Preview of Train and test data....\n",
        "print(\"Train Dataset:\")\n",
        "\n",
        "print(\"Training features: \")\n",
        "print(\" \")\n",
        "print(X_train)\n",
        "print(\" \")\n",
        "\n",
        "print(\"Training labels: \")\n",
        "print(\" \")\n",
        "print(y_train)\n",
        "print(\" \")\n",
        "print(\" \")\n",
        "\n",
        "\n",
        "print(\"Test Dataset:\")\n",
        "\n",
        "print(\"Testing features: \")\n",
        "print(\" \")\n",
        "print(X_test)\n",
        "print(\" \")\n",
        "\n",
        "print(\"Testing labels: \")\n",
        "print(\" \")\n",
        "print(y_test)\n",
        "print(\" \")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Dataset:\n",
            "Training features: \n",
            " \n",
            "[[-0.03393827  1.22318145 -0.87555676 ... -0.30766231 -0.25201915\n",
            "  -0.89776301]\n",
            " [-0.95027145  1.22318145 -0.87555676 ... -0.30766231 -0.25201915\n",
            "  -0.89776301]\n",
            " [ 0.88239492  0.07050037 -0.87555676 ... -0.30766231 -0.25201915\n",
            "  -0.89776301]\n",
            " ...\n",
            " [ 0.88239492  1.22318145  0.23035034 ... -0.30766231 -0.25201915\n",
            "  -0.89776301]\n",
            " [ 0.88239492  0.07050037  0.23035034 ... -0.30766231 -0.25201915\n",
            "  -0.89776301]\n",
            " [ 0.88239492  0.07050037  0.23035034 ... -0.30766231 -0.25201915\n",
            "  -0.89776301]]\n",
            " \n",
            "Training labels: \n",
            " \n",
            "[0 0 0 ... 0 0 0]\n",
            " \n",
            " \n",
            "Test Dataset:\n",
            "Testing features: \n",
            " \n",
            "[[-0.14868915 -1.15692352 -0.91706245 ... -0.26854879  2.72802565\n",
            "  -0.89397743]\n",
            " [-0.97025886 -1.15692352 -0.91706245 ... -0.26854879  2.72802565\n",
            "  -0.89397743]\n",
            " [-0.14868915 -0.04749316  0.11574703 ... -0.26854879 -0.24822652\n",
            "  -0.13248664]\n",
            " ...\n",
            " [-0.97025886 -1.15692352 -0.91706245 ... -0.26854879 -0.24822652\n",
            "  -0.89397743]\n",
            " [-0.97025886 -1.15692352  0.11574703 ... -0.26854879 -0.24822652\n",
            "  -0.89397743]\n",
            " [-0.97025886 -0.04749316 -0.91706245 ... -0.26854879 -0.24822652\n",
            "  -0.89397743]]\n",
            " \n",
            "Testing labels: \n",
            " \n",
            "[1 1 1 ... 0 0 0]\n",
            " \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qX1rKjBUO2_D",
        "outputId": "befac850-ce98-4ed1-a348-7504e4087cee"
      },
      "source": [
        "model = tf.keras.models.load_model('best_model (1).h5')\n",
        "scores = new_model.evaluate(X_test, y_test, verbose=0)\n",
        "print(scores)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.015783248469233513, 0.9833333492279053]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbJ7jdOXPIIK",
        "outputId": "6b1f3ffa-0e4a-4518-b725-17b5d98e6b02"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 15)                810       \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 20)                320       \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 21        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,151\n",
            "Trainable params: 1,151\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwLzHAYcY49E"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(30,input_dim=53,activation='sigmoid'))\n",
        "model.add(Dense(40,activation='sigmoid'))\n",
        "model.add(Dense(10,activation='sigmoid'))\n",
        "model.add(Dense(1,activation='sigmoid'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1rL96AdZDF8"
      },
      "source": [
        "model.compile(optimizer='Adam',loss='mse',metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YV0YDpnJZE00"
      },
      "source": [
        "import h5py\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min',  verbose=1, patience=100, baseline=0.4, save_best_only=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-mDQkp9uzND",
        "outputId": "091d37e6-bf9b-4e2d-c137-788788f646fb"
      },
      "source": [
        "import time\n",
        "t0=time.time()\n",
        "history = model.fit(X_res, y_res,validation_data=(X_test,y_test),batch_size=32,epochs=500, callbacks=mc)\n",
        "print (\"training time:\", round(time.time()-t0, 3), \"s\") # the time would be round to 3 decimal in seconds\n",
        "t1=time.time()\n",
        "y_pred = model.predict(X_test)\n",
        "print (\"predict time:\", round(time.time()-t1, 3), \"s\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "281/298 [===========================>..] - ETA: 0s - loss: 0.1192 - accuracy: 0.8754\n",
            "Epoch 00001: val_loss improved from inf to 0.04964, saving model to best_model.h5\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.1144 - accuracy: 0.8811 - val_loss: 0.0496 - val_accuracy: 0.9373\n",
            "Epoch 2/500\n",
            "289/298 [============================>.] - ETA: 0s - loss: 0.0252 - accuracy: 0.9770\n",
            "Epoch 00002: val_loss improved from 0.04964 to 0.02983, saving model to best_model.h5\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0250 - accuracy: 0.9771 - val_loss: 0.0298 - val_accuracy: 0.9619\n",
            "Epoch 3/500\n",
            "276/298 [==========================>...] - ETA: 0s - loss: 0.0169 - accuracy: 0.9828\n",
            "Epoch 00003: val_loss improved from 0.02983 to 0.02333, saving model to best_model.h5\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0169 - accuracy: 0.9826 - val_loss: 0.0233 - val_accuracy: 0.9675\n",
            "Epoch 4/500\n",
            "296/298 [============================>.] - ETA: 0s - loss: 0.0147 - accuracy: 0.9852\n",
            "Epoch 00004: val_loss improved from 0.02333 to 0.02322, saving model to best_model.h5\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0146 - accuracy: 0.9853 - val_loss: 0.0232 - val_accuracy: 0.9714\n",
            "Epoch 5/500\n",
            "287/298 [===========================>..] - ETA: 0s - loss: 0.0133 - accuracy: 0.9851\n",
            "Epoch 00005: val_loss improved from 0.02322 to 0.02169, saving model to best_model.h5\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0135 - accuracy: 0.9848 - val_loss: 0.0217 - val_accuracy: 0.9730\n",
            "Epoch 6/500\n",
            "271/298 [==========================>...] - ETA: 0s - loss: 0.0127 - accuracy: 0.9860\n",
            "Epoch 00006: val_loss did not improve from 0.02169\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0128 - accuracy: 0.9859 - val_loss: 0.0237 - val_accuracy: 0.9690\n",
            "Epoch 7/500\n",
            "297/298 [============================>.] - ETA: 0s - loss: 0.0123 - accuracy: 0.9868\n",
            "Epoch 00007: val_loss did not improve from 0.02169\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0123 - accuracy: 0.9869 - val_loss: 0.0233 - val_accuracy: 0.9698\n",
            "Epoch 8/500\n",
            "294/298 [============================>.] - ETA: 0s - loss: 0.0119 - accuracy: 0.9866\n",
            "Epoch 00008: val_loss improved from 0.02169 to 0.01985, saving model to best_model.h5\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0118 - accuracy: 0.9867 - val_loss: 0.0199 - val_accuracy: 0.9738\n",
            "Epoch 9/500\n",
            "286/298 [===========================>..] - ETA: 0s - loss: 0.0115 - accuracy: 0.9871\n",
            "Epoch 00009: val_loss did not improve from 0.01985\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0115 - accuracy: 0.9872 - val_loss: 0.0241 - val_accuracy: 0.9706\n",
            "Epoch 10/500\n",
            "294/298 [============================>.] - ETA: 0s - loss: 0.0111 - accuracy: 0.9874\n",
            "Epoch 00010: val_loss did not improve from 0.01985\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0111 - accuracy: 0.9873 - val_loss: 0.0215 - val_accuracy: 0.9722\n",
            "Epoch 11/500\n",
            "276/298 [==========================>...] - ETA: 0s - loss: 0.0110 - accuracy: 0.9877\n",
            "Epoch 00011: val_loss did not improve from 0.01985\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0107 - accuracy: 0.9879 - val_loss: 0.0255 - val_accuracy: 0.9690\n",
            "Epoch 12/500\n",
            "274/298 [==========================>...] - ETA: 0s - loss: 0.0103 - accuracy: 0.9881\n",
            "Epoch 00012: val_loss did not improve from 0.01985\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0104 - accuracy: 0.9881 - val_loss: 0.0252 - val_accuracy: 0.9683\n",
            "Epoch 13/500\n",
            "280/298 [===========================>..] - ETA: 0s - loss: 0.0105 - accuracy: 0.9882\n",
            "Epoch 00013: val_loss did not improve from 0.01985\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0104 - accuracy: 0.9883 - val_loss: 0.0207 - val_accuracy: 0.9754\n",
            "Epoch 14/500\n",
            "297/298 [============================>.] - ETA: 0s - loss: 0.0104 - accuracy: 0.9882\n",
            "Epoch 00014: val_loss did not improve from 0.01985\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0104 - accuracy: 0.9882 - val_loss: 0.0225 - val_accuracy: 0.9730\n",
            "Epoch 15/500\n",
            "296/298 [============================>.] - ETA: 0s - loss: 0.0098 - accuracy: 0.9890\n",
            "Epoch 00015: val_loss improved from 0.01985 to 0.01853, saving model to best_model.h5\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0098 - accuracy: 0.9891 - val_loss: 0.0185 - val_accuracy: 0.9786\n",
            "Epoch 16/500\n",
            "289/298 [============================>.] - ETA: 0s - loss: 0.0102 - accuracy: 0.9881\n",
            "Epoch 00016: val_loss did not improve from 0.01853\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0100 - accuracy: 0.9883 - val_loss: 0.0211 - val_accuracy: 0.9762\n",
            "Epoch 17/500\n",
            "294/298 [============================>.] - ETA: 0s - loss: 0.0094 - accuracy: 0.9898\n",
            "Epoch 00017: val_loss did not improve from 0.01853\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0094 - accuracy: 0.9898 - val_loss: 0.0200 - val_accuracy: 0.9778\n",
            "Epoch 18/500\n",
            "279/298 [===========================>..] - ETA: 0s - loss: 0.0091 - accuracy: 0.9896\n",
            "Epoch 00018: val_loss improved from 0.01853 to 0.01780, saving model to best_model.h5\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0094 - accuracy: 0.9893 - val_loss: 0.0178 - val_accuracy: 0.9794\n",
            "Epoch 19/500\n",
            "272/298 [==========================>...] - ETA: 0s - loss: 0.0090 - accuracy: 0.9902\n",
            "Epoch 00019: val_loss did not improve from 0.01780\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0091 - accuracy: 0.9901 - val_loss: 0.0241 - val_accuracy: 0.9714\n",
            "Epoch 20/500\n",
            "284/298 [===========================>..] - ETA: 0s - loss: 0.0087 - accuracy: 0.9901\n",
            "Epoch 00020: val_loss did not improve from 0.01780\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0091 - accuracy: 0.9897 - val_loss: 0.0182 - val_accuracy: 0.9778\n",
            "Epoch 21/500\n",
            "283/298 [===========================>..] - ETA: 0s - loss: 0.0090 - accuracy: 0.9898\n",
            "Epoch 00021: val_loss did not improve from 0.01780\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0089 - accuracy: 0.9900 - val_loss: 0.0242 - val_accuracy: 0.9738\n",
            "Epoch 22/500\n",
            "277/298 [==========================>...] - ETA: 0s - loss: 0.0083 - accuracy: 0.9913\n",
            "Epoch 00022: val_loss did not improve from 0.01780\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0085 - accuracy: 0.9912 - val_loss: 0.0233 - val_accuracy: 0.9754\n",
            "Epoch 23/500\n",
            "298/298 [==============================] - ETA: 0s - loss: 0.0088 - accuracy: 0.9901\n",
            "Epoch 00023: val_loss did not improve from 0.01780\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0088 - accuracy: 0.9901 - val_loss: 0.0225 - val_accuracy: 0.9754\n",
            "Epoch 24/500\n",
            "289/298 [============================>.] - ETA: 0s - loss: 0.0090 - accuracy: 0.9906\n",
            "Epoch 00024: val_loss did not improve from 0.01780\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0087 - accuracy: 0.9909 - val_loss: 0.0203 - val_accuracy: 0.9778\n",
            "Epoch 25/500\n",
            "297/298 [============================>.] - ETA: 0s - loss: 0.0085 - accuracy: 0.9910\n",
            "Epoch 00025: val_loss did not improve from 0.01780\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0085 - accuracy: 0.9910 - val_loss: 0.0228 - val_accuracy: 0.9762\n",
            "Epoch 26/500\n",
            "279/298 [===========================>..] - ETA: 0s - loss: 0.0082 - accuracy: 0.9916\n",
            "Epoch 00026: val_loss improved from 0.01780 to 0.01775, saving model to best_model.h5\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0082 - accuracy: 0.9915 - val_loss: 0.0178 - val_accuracy: 0.9794\n",
            "Epoch 27/500\n",
            "283/298 [===========================>..] - ETA: 0s - loss: 0.0083 - accuracy: 0.9913\n",
            "Epoch 00027: val_loss did not improve from 0.01775\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0082 - accuracy: 0.9913 - val_loss: 0.0178 - val_accuracy: 0.9802\n",
            "Epoch 28/500\n",
            "296/298 [============================>.] - ETA: 0s - loss: 0.0080 - accuracy: 0.9919\n",
            "Epoch 00028: val_loss did not improve from 0.01775\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0080 - accuracy: 0.9919 - val_loss: 0.0192 - val_accuracy: 0.9778\n",
            "Epoch 29/500\n",
            "297/298 [============================>.] - ETA: 0s - loss: 0.0079 - accuracy: 0.9917\n",
            "Epoch 00029: val_loss did not improve from 0.01775\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0079 - accuracy: 0.9917 - val_loss: 0.0215 - val_accuracy: 0.9770\n",
            "Epoch 30/500\n",
            "281/298 [===========================>..] - ETA: 0s - loss: 0.0079 - accuracy: 0.9915\n",
            "Epoch 00030: val_loss did not improve from 0.01775\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0079 - accuracy: 0.9916 - val_loss: 0.0181 - val_accuracy: 0.9802\n",
            "Epoch 31/500\n",
            "291/298 [============================>.] - ETA: 0s - loss: 0.0077 - accuracy: 0.9921\n",
            "Epoch 00031: val_loss did not improve from 0.01775\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0077 - accuracy: 0.9921 - val_loss: 0.0213 - val_accuracy: 0.9762\n",
            "Epoch 32/500\n",
            "295/298 [============================>.] - ETA: 0s - loss: 0.0078 - accuracy: 0.9918\n",
            "Epoch 00032: val_loss improved from 0.01775 to 0.01760, saving model to best_model.h5\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0077 - accuracy: 0.9919 - val_loss: 0.0176 - val_accuracy: 0.9794\n",
            "Epoch 33/500\n",
            "270/298 [==========================>...] - ETA: 0s - loss: 0.0079 - accuracy: 0.9919\n",
            "Epoch 00033: val_loss did not improve from 0.01760\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0079 - accuracy: 0.9918 - val_loss: 0.0229 - val_accuracy: 0.9754\n",
            "Epoch 34/500\n",
            "279/298 [===========================>..] - ETA: 0s - loss: 0.0071 - accuracy: 0.9926\n",
            "Epoch 00034: val_loss did not improve from 0.01760\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0075 - accuracy: 0.9923 - val_loss: 0.0207 - val_accuracy: 0.9770\n",
            "Epoch 35/500\n",
            "291/298 [============================>.] - ETA: 0s - loss: 0.0074 - accuracy: 0.9926\n",
            "Epoch 00035: val_loss improved from 0.01760 to 0.01616, saving model to best_model.h5\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0074 - accuracy: 0.9927 - val_loss: 0.0162 - val_accuracy: 0.9817\n",
            "Epoch 36/500\n",
            "279/298 [===========================>..] - ETA: 0s - loss: 0.0072 - accuracy: 0.9926\n",
            "Epoch 00036: val_loss did not improve from 0.01616\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0073 - accuracy: 0.9925 - val_loss: 0.0202 - val_accuracy: 0.9778\n",
            "Epoch 37/500\n",
            "293/298 [============================>.] - ETA: 0s - loss: 0.0072 - accuracy: 0.9923\n",
            "Epoch 00037: val_loss did not improve from 0.01616\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0073 - accuracy: 0.9922 - val_loss: 0.0197 - val_accuracy: 0.9786\n",
            "Epoch 38/500\n",
            "275/298 [==========================>...] - ETA: 0s - loss: 0.0070 - accuracy: 0.9927\n",
            "Epoch 00038: val_loss did not improve from 0.01616\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0071 - accuracy: 0.9927 - val_loss: 0.0187 - val_accuracy: 0.9794\n",
            "Epoch 39/500\n",
            "295/298 [============================>.] - ETA: 0s - loss: 0.0073 - accuracy: 0.9925\n",
            "Epoch 00039: val_loss did not improve from 0.01616\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0073 - accuracy: 0.9924 - val_loss: 0.0173 - val_accuracy: 0.9802\n",
            "Epoch 40/500\n",
            "280/298 [===========================>..] - ETA: 0s - loss: 0.0065 - accuracy: 0.9930\n",
            "Epoch 00040: val_loss did not improve from 0.01616\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0071 - accuracy: 0.9922 - val_loss: 0.0222 - val_accuracy: 0.9754\n",
            "Epoch 41/500\n",
            "288/298 [===========================>..] - ETA: 0s - loss: 0.0067 - accuracy: 0.9932\n",
            "Epoch 00041: val_loss did not improve from 0.01616\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0069 - accuracy: 0.9929 - val_loss: 0.0254 - val_accuracy: 0.9714\n",
            "Epoch 42/500\n",
            "295/298 [============================>.] - ETA: 0s - loss: 0.0067 - accuracy: 0.9931\n",
            "Epoch 00042: val_loss did not improve from 0.01616\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0066 - accuracy: 0.9932 - val_loss: 0.0207 - val_accuracy: 0.9762\n",
            "Epoch 43/500\n",
            "290/298 [============================>.] - ETA: 0s - loss: 0.0063 - accuracy: 0.9937\n",
            "Epoch 00043: val_loss did not improve from 0.01616\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0066 - accuracy: 0.9935 - val_loss: 0.0203 - val_accuracy: 0.9762\n",
            "Epoch 44/500\n",
            "292/298 [============================>.] - ETA: 0s - loss: 0.0064 - accuracy: 0.9935\n",
            "Epoch 00044: val_loss did not improve from 0.01616\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0063 - accuracy: 0.9936 - val_loss: 0.0201 - val_accuracy: 0.9770\n",
            "Epoch 45/500\n",
            "297/298 [============================>.] - ETA: 0s - loss: 0.0065 - accuracy: 0.9931\n",
            "Epoch 00045: val_loss did not improve from 0.01616\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0065 - accuracy: 0.9931 - val_loss: 0.0236 - val_accuracy: 0.9738\n",
            "Epoch 46/500\n",
            "288/298 [===========================>..] - ETA: 0s - loss: 0.0059 - accuracy: 0.9941\n",
            "Epoch 00046: val_loss did not improve from 0.01616\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.0061 - accuracy: 0.9939 - val_loss: 0.0281 - val_accuracy: 0.9667\n",
            "Epoch 47/500\n",
            "289/298 [============================>.] - ETA: 0s - loss: 0.0061 - accuracy: 0.9937\n",
            "Epoch 00047: val_loss did not improve from 0.01616\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0061 - accuracy: 0.9936 - val_loss: 0.0254 - val_accuracy: 0.9690\n",
            "Epoch 48/500\n",
            "287/298 [===========================>..] - ETA: 0s - loss: 0.0059 - accuracy: 0.9941\n",
            "Epoch 00048: val_loss did not improve from 0.01616\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0059 - accuracy: 0.9941 - val_loss: 0.0210 - val_accuracy: 0.9754\n",
            "Epoch 49/500\n",
            "297/298 [============================>.] - ETA: 0s - loss: 0.0061 - accuracy: 0.9940\n",
            "Epoch 00049: val_loss did not improve from 0.01616\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0061 - accuracy: 0.9940 - val_loss: 0.0210 - val_accuracy: 0.9762\n",
            "Epoch 50/500\n",
            "298/298 [==============================] - ETA: 0s - loss: 0.0058 - accuracy: 0.9943\n",
            "Epoch 00050: val_loss did not improve from 0.01616\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.0058 - accuracy: 0.9943 - val_loss: 0.0256 - val_accuracy: 0.9714\n",
            "Epoch 51/500\n",
            "278/298 [==========================>...] - ETA: 0s - loss: 0.0062 - accuracy: 0.9938\n",
            "Epoch 00051: val_loss did not improve from 0.01616\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0059 - accuracy: 0.9940 - val_loss: 0.0217 - val_accuracy: 0.9730\n",
            "Epoch 52/500\n",
            "292/298 [============================>.] - ETA: 0s - loss: 0.0061 - accuracy: 0.9936\n",
            "Epoch 00052: val_loss did not improve from 0.01616\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0061 - accuracy: 0.9936 - val_loss: 0.0242 - val_accuracy: 0.9730\n",
            "Epoch 53/500\n",
            "288/298 [===========================>..] - ETA: 0s - loss: 0.0058 - accuracy: 0.9941\n",
            "Epoch 00053: val_loss did not improve from 0.01616\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0059 - accuracy: 0.9941 - val_loss: 0.0218 - val_accuracy: 0.9754\n",
            "Epoch 54/500\n",
            "285/298 [===========================>..] - ETA: 0s - loss: 0.0058 - accuracy: 0.9945\n",
            "Epoch 00054: val_loss did not improve from 0.01616\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0058 - accuracy: 0.9945 - val_loss: 0.0223 - val_accuracy: 0.9746\n",
            "Epoch 55/500\n",
            "276/298 [==========================>...] - ETA: 0s - loss: 0.0059 - accuracy: 0.9941\n",
            "Epoch 00055: val_loss did not improve from 0.01616\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0056 - accuracy: 0.9944 - val_loss: 0.0214 - val_accuracy: 0.9754\n",
            "Epoch 56/500\n",
            "278/298 [==========================>...] - ETA: 0s - loss: 0.0056 - accuracy: 0.9945\n",
            "Epoch 00056: val_loss did not improve from 0.01616\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0055 - accuracy: 0.9945 - val_loss: 0.0249 - val_accuracy: 0.9722\n",
            "Epoch 57/500\n",
            "275/298 [==========================>...] - ETA: 0s - loss: 0.0052 - accuracy: 0.9948\n",
            "Epoch 00057: val_loss did not improve from 0.01616\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0059 - accuracy: 0.9941 - val_loss: 0.0197 - val_accuracy: 0.9778\n",
            "Epoch 58/500\n",
            "287/298 [===========================>..] - ETA: 0s - loss: 0.0052 - accuracy: 0.9949\n",
            "Epoch 00058: val_loss did not improve from 0.01616\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0056 - accuracy: 0.9945 - val_loss: 0.0249 - val_accuracy: 0.9714\n",
            "Epoch 59/500\n",
            "283/298 [===========================>..] - ETA: 0s - loss: 0.0059 - accuracy: 0.9940\n",
            "Epoch 00059: val_loss did not improve from 0.01616\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0056 - accuracy: 0.9943 - val_loss: 0.0267 - val_accuracy: 0.9690\n",
            "Epoch 60/500\n",
            "288/298 [===========================>..] - ETA: 0s - loss: 0.0056 - accuracy: 0.9944\n",
            "Epoch 00060: val_loss did not improve from 0.01616\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0056 - accuracy: 0.9943 - val_loss: 0.0260 - val_accuracy: 0.9738\n",
            "Epoch 61/500\n",
            "297/298 [============================>.] - ETA: 0s - loss: 0.0055 - accuracy: 0.9945\n",
            "Epoch 00061: val_loss did not improve from 0.01616\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0055 - accuracy: 0.9945 - val_loss: 0.0218 - val_accuracy: 0.9754\n",
            "Epoch 62/500\n",
            "293/298 [============================>.] - ETA: 0s - loss: 0.0055 - accuracy: 0.9946\n",
            "Epoch 00062: val_loss did not improve from 0.01616\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0054 - accuracy: 0.9946 - val_loss: 0.0211 - val_accuracy: 0.9778\n",
            "Epoch 63/500\n",
            "281/298 [===========================>..] - ETA: 0s - loss: 0.0051 - accuracy: 0.9950\n",
            "Epoch 00063: val_loss did not improve from 0.01616\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0054 - accuracy: 0.9946 - val_loss: 0.0233 - val_accuracy: 0.9738\n",
            "Epoch 64/500\n",
            "296/298 [============================>.] - ETA: 0s - loss: 0.0056 - accuracy: 0.9944\n",
            "Epoch 00064: val_loss did not improve from 0.01616\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0056 - accuracy: 0.9943 - val_loss: 0.0227 - val_accuracy: 0.9754\n",
            "Epoch 65/500\n",
            "283/298 [===========================>..] - ETA: 0s - loss: 0.0054 - accuracy: 0.9945\n",
            "Epoch 00065: val_loss did not improve from 0.01616\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0055 - accuracy: 0.9942 - val_loss: 0.0207 - val_accuracy: 0.9770\n",
            "Epoch 66/500\n",
            "293/298 [============================>.] - ETA: 0s - loss: 0.0055 - accuracy: 0.9946\n",
            "Epoch 00066: val_loss did not improve from 0.01616\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0054 - accuracy: 0.9946 - val_loss: 0.0229 - val_accuracy: 0.9754\n",
            "Epoch 67/500\n",
            "288/298 [===========================>..] - ETA: 0s - loss: 0.0054 - accuracy: 0.9947\n",
            "Epoch 00067: val_loss did not improve from 0.01616\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0056 - accuracy: 0.9945 - val_loss: 0.0233 - val_accuracy: 0.9746\n",
            "Epoch 68/500\n",
            "293/298 [============================>.] - ETA: 0s - loss: 0.0057 - accuracy: 0.9942\n",
            "Epoch 00068: val_loss did not improve from 0.01616\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0056 - accuracy: 0.9943 - val_loss: 0.0214 - val_accuracy: 0.9754\n",
            "Epoch 69/500\n",
            "283/298 [===========================>..] - ETA: 0s - loss: 0.0055 - accuracy: 0.9945\n",
            "Epoch 00069: val_loss did not improve from 0.01616\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0054 - accuracy: 0.9946 - val_loss: 0.0232 - val_accuracy: 0.9754\n",
            "Epoch 70/500\n",
            "293/298 [============================>.] - ETA: 0s - loss: 0.0054 - accuracy: 0.9947\n",
            "Epoch 00070: val_loss did not improve from 0.01616\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0053 - accuracy: 0.9948 - val_loss: 0.0231 - val_accuracy: 0.9754\n",
            "Epoch 71/500\n",
            "290/298 [============================>.] - ETA: 0s - loss: 0.0054 - accuracy: 0.9946\n",
            "Epoch 00071: val_loss did not improve from 0.01616\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0053 - accuracy: 0.9948 - val_loss: 0.0244 - val_accuracy: 0.9746\n",
            "Epoch 72/500\n",
            "281/298 [===========================>..] - ETA: 0s - loss: 0.0051 - accuracy: 0.9949\n",
            "Epoch 00072: val_loss did not improve from 0.01616\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0053 - accuracy: 0.9948 - val_loss: 0.0229 - val_accuracy: 0.9754\n",
            "Epoch 73/500\n",
            "284/298 [===========================>..] - ETA: 0s - loss: 0.0050 - accuracy: 0.9950\n",
            "Epoch 00073: val_loss did not improve from 0.01616\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0052 - accuracy: 0.9948 - val_loss: 0.0230 - val_accuracy: 0.9754\n",
            "Epoch 74/500\n",
            "279/298 [===========================>..] - ETA: 0s - loss: 0.0053 - accuracy: 0.9947\n",
            "Epoch 00074: val_loss did not improve from 0.01616\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0052 - accuracy: 0.9948 - val_loss: 0.0244 - val_accuracy: 0.9754\n",
            "Epoch 75/500\n",
            "272/298 [==========================>...] - ETA: 0s - loss: 0.0054 - accuracy: 0.9945\n",
            "Epoch 00075: val_loss did not improve from 0.01616\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0056 - accuracy: 0.9941 - val_loss: 0.0232 - val_accuracy: 0.9746\n",
            "Epoch 76/500\n",
            "279/298 [===========================>..] - ETA: 0s - loss: 0.0056 - accuracy: 0.9940\n",
            "Epoch 00076: val_loss did not improve from 0.01616\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0057 - accuracy: 0.9939 - val_loss: 0.0206 - val_accuracy: 0.9770\n",
            "Epoch 77/500\n",
            "285/298 [===========================>..] - ETA: 0s - loss: 0.0052 - accuracy: 0.9947\n",
            "Epoch 00077: val_loss did not improve from 0.01616\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0052 - accuracy: 0.9948 - val_loss: 0.0226 - val_accuracy: 0.9754\n",
            "Epoch 78/500\n",
            "293/298 [============================>.] - ETA: 0s - loss: 0.0052 - accuracy: 0.9949\n",
            "Epoch 00078: val_loss did not improve from 0.01616\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0051 - accuracy: 0.9950 - val_loss: 0.0210 - val_accuracy: 0.9778\n",
            "Epoch 79/500\n",
            "286/298 [===========================>..] - ETA: 0s - loss: 0.0053 - accuracy: 0.9943\n",
            "Epoch 00079: val_loss did not improve from 0.01616\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0054 - accuracy: 0.9942 - val_loss: 0.0220 - val_accuracy: 0.9754\n",
            "Epoch 80/500\n",
            "285/298 [===========================>..] - ETA: 0s - loss: 0.0050 - accuracy: 0.9951\n",
            "Epoch 00080: val_loss did not improve from 0.01616\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0050 - accuracy: 0.9951 - val_loss: 0.0252 - val_accuracy: 0.9738\n",
            "Epoch 81/500\n",
            "289/298 [============================>.] - ETA: 0s - loss: 0.0051 - accuracy: 0.9949\n",
            "Epoch 00081: val_loss did not improve from 0.01616\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0051 - accuracy: 0.9950 - val_loss: 0.0216 - val_accuracy: 0.9770\n",
            "Epoch 82/500\n",
            "282/298 [===========================>..] - ETA: 0s - loss: 0.0051 - accuracy: 0.9949\n",
            "Epoch 00082: val_loss did not improve from 0.01616\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0049 - accuracy: 0.9951 - val_loss: 0.0239 - val_accuracy: 0.9738\n",
            "Epoch 83/500\n",
            "279/298 [===========================>..] - ETA: 0s - loss: 0.0052 - accuracy: 0.9948\n",
            "Epoch 00083: val_loss did not improve from 0.01616\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0050 - accuracy: 0.9951 - val_loss: 0.0211 - val_accuracy: 0.9778\n",
            "Epoch 84/500\n",
            "283/298 [===========================>..] - ETA: 0s - loss: 0.0050 - accuracy: 0.9950\n",
            "Epoch 00084: val_loss did not improve from 0.01616\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0049 - accuracy: 0.9951 - val_loss: 0.0228 - val_accuracy: 0.9762\n",
            "Epoch 85/500\n",
            "291/298 [============================>.] - ETA: 0s - loss: 0.0049 - accuracy: 0.9951\n",
            "Epoch 00085: val_loss did not improve from 0.01616\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0049 - accuracy: 0.9951 - val_loss: 0.0235 - val_accuracy: 0.9746\n",
            "Epoch 86/500\n",
            "279/298 [===========================>..] - ETA: 0s - loss: 0.0049 - accuracy: 0.9951\n",
            "Epoch 00086: val_loss did not improve from 0.01616\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0050 - accuracy: 0.9951 - val_loss: 0.0217 - val_accuracy: 0.9778\n",
            "Epoch 87/500\n",
            "291/298 [============================>.] - ETA: 0s - loss: 0.0050 - accuracy: 0.9950\n",
            "Epoch 00087: val_loss did not improve from 0.01616\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0049 - accuracy: 0.9951 - val_loss: 0.0214 - val_accuracy: 0.9770\n",
            "Epoch 88/500\n",
            "291/298 [============================>.] - ETA: 0s - loss: 0.0049 - accuracy: 0.9951\n",
            "Epoch 00088: val_loss did not improve from 0.01616\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0049 - accuracy: 0.9951 - val_loss: 0.0211 - val_accuracy: 0.9770\n",
            "Epoch 89/500\n",
            "276/298 [==========================>...] - ETA: 0s - loss: 0.0051 - accuracy: 0.9948\n",
            "Epoch 00089: val_loss did not improve from 0.01616\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0064 - accuracy: 0.9934 - val_loss: 0.0553 - val_accuracy: 0.9421\n",
            "Epoch 90/500\n",
            "280/298 [===========================>..] - ETA: 0s - loss: 0.0073 - accuracy: 0.9922\n",
            "Epoch 00090: val_loss did not improve from 0.01616\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0070 - accuracy: 0.9925 - val_loss: 0.0206 - val_accuracy: 0.9794\n",
            "Epoch 91/500\n",
            "280/298 [===========================>..] - ETA: 0s - loss: 0.0052 - accuracy: 0.9946\n",
            "Epoch 00091: val_loss did not improve from 0.01616\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0051 - accuracy: 0.9948 - val_loss: 0.0204 - val_accuracy: 0.9794\n",
            "Epoch 92/500\n",
            "295/298 [============================>.] - ETA: 0s - loss: 0.0050 - accuracy: 0.9950\n",
            "Epoch 00092: val_loss did not improve from 0.01616\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0049 - accuracy: 0.9951 - val_loss: 0.0199 - val_accuracy: 0.9794\n",
            "Epoch 93/500\n",
            "289/298 [============================>.] - ETA: 0s - loss: 0.0050 - accuracy: 0.9950\n",
            "Epoch 00093: val_loss did not improve from 0.01616\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0049 - accuracy: 0.9951 - val_loss: 0.0203 - val_accuracy: 0.9794\n",
            "Epoch 94/500\n",
            "282/298 [===========================>..] - ETA: 0s - loss: 0.0051 - accuracy: 0.9949\n",
            "Epoch 00094: val_loss did not improve from 0.01616\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0049 - accuracy: 0.9951 - val_loss: 0.0203 - val_accuracy: 0.9794\n",
            "Epoch 95/500\n",
            "278/298 [==========================>...] - ETA: 0s - loss: 0.0052 - accuracy: 0.9948\n",
            "Epoch 00095: val_loss did not improve from 0.01616\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0049 - accuracy: 0.9951 - val_loss: 0.0200 - val_accuracy: 0.9802\n",
            "Epoch 96/500\n",
            "276/298 [==========================>...] - ETA: 0s - loss: 0.0049 - accuracy: 0.9951\n",
            "Epoch 00096: val_loss did not improve from 0.01616\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0049 - accuracy: 0.9951 - val_loss: 0.0198 - val_accuracy: 0.9802\n",
            "Epoch 97/500\n",
            "292/298 [============================>.] - ETA: 0s - loss: 0.0049 - accuracy: 0.9951\n",
            "Epoch 00097: val_loss did not improve from 0.01616\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0049 - accuracy: 0.9951 - val_loss: 0.0204 - val_accuracy: 0.9794\n",
            "Epoch 98/500\n",
            "275/298 [==========================>...] - ETA: 0s - loss: 0.0048 - accuracy: 0.9952\n",
            "Epoch 00098: val_loss did not improve from 0.01616\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0049 - accuracy: 0.9951 - val_loss: 0.0203 - val_accuracy: 0.9794\n",
            "Epoch 99/500\n",
            "298/298 [==============================] - ETA: 0s - loss: 0.0049 - accuracy: 0.9951\n",
            "Epoch 00099: val_loss did not improve from 0.01616\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0049 - accuracy: 0.9951 - val_loss: 0.0202 - val_accuracy: 0.9794\n",
            "Epoch 100/500\n",
            "287/298 [===========================>..] - ETA: 0s - loss: 0.0049 - accuracy: 0.9951\n",
            "Epoch 00100: val_loss did not improve from 0.01616\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0049 - accuracy: 0.9951 - val_loss: 0.0209 - val_accuracy: 0.9786\n",
            "Epoch 101/500\n",
            "295/298 [============================>.] - ETA: 0s - loss: 0.0050 - accuracy: 0.9950\n",
            "Epoch 00101: val_loss did not improve from 0.01616\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0049 - accuracy: 0.9951 - val_loss: 0.0198 - val_accuracy: 0.9802\n",
            "Epoch 102/500\n",
            "279/298 [===========================>..] - ETA: 0s - loss: 0.0050 - accuracy: 0.9950\n",
            "Epoch 00102: val_loss did not improve from 0.01616\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0049 - accuracy: 0.9951 - val_loss: 0.0202 - val_accuracy: 0.9794\n",
            "Epoch 103/500\n",
            "288/298 [===========================>..] - ETA: 0s - loss: 0.0050 - accuracy: 0.9950\n",
            "Epoch 00103: val_loss did not improve from 0.01616\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0049 - accuracy: 0.9951 - val_loss: 0.0205 - val_accuracy: 0.9786\n",
            "Epoch 104/500\n",
            "273/298 [==========================>...] - ETA: 0s - loss: 0.0058 - accuracy: 0.9940\n",
            "Epoch 00104: val_loss improved from 0.01616 to 0.01578, saving model to best_model.h5\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0058 - accuracy: 0.9941 - val_loss: 0.0158 - val_accuracy: 0.9833\n",
            "Epoch 105/500\n",
            "288/298 [===========================>..] - ETA: 0s - loss: 0.0054 - accuracy: 0.9945\n",
            "Epoch 00105: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0054 - accuracy: 0.9944 - val_loss: 0.0246 - val_accuracy: 0.9730\n",
            "Epoch 106/500\n",
            "276/298 [==========================>...] - ETA: 0s - loss: 0.0053 - accuracy: 0.9946\n",
            "Epoch 00106: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0051 - accuracy: 0.9948 - val_loss: 0.0203 - val_accuracy: 0.9794\n",
            "Epoch 107/500\n",
            "297/298 [============================>.] - ETA: 0s - loss: 0.0049 - accuracy: 0.9951\n",
            "Epoch 00107: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0049 - accuracy: 0.9951 - val_loss: 0.0206 - val_accuracy: 0.9794\n",
            "Epoch 108/500\n",
            "287/298 [===========================>..] - ETA: 0s - loss: 0.0049 - accuracy: 0.9951\n",
            "Epoch 00108: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0049 - accuracy: 0.9951 - val_loss: 0.0206 - val_accuracy: 0.9794\n",
            "Epoch 109/500\n",
            "288/298 [===========================>..] - ETA: 0s - loss: 0.0050 - accuracy: 0.9950\n",
            "Epoch 00109: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0049 - accuracy: 0.9951 - val_loss: 0.0207 - val_accuracy: 0.9786\n",
            "Epoch 110/500\n",
            "290/298 [============================>.] - ETA: 0s - loss: 0.0049 - accuracy: 0.9950\n",
            "Epoch 00110: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0049 - accuracy: 0.9951 - val_loss: 0.0207 - val_accuracy: 0.9786\n",
            "Epoch 111/500\n",
            "288/298 [===========================>..] - ETA: 0s - loss: 0.0049 - accuracy: 0.9951\n",
            "Epoch 00111: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0049 - accuracy: 0.9951 - val_loss: 0.0205 - val_accuracy: 0.9794\n",
            "Epoch 112/500\n",
            "292/298 [============================>.] - ETA: 0s - loss: 0.0049 - accuracy: 0.9951\n",
            "Epoch 00112: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0049 - accuracy: 0.9951 - val_loss: 0.0206 - val_accuracy: 0.9786\n",
            "Epoch 113/500\n",
            "284/298 [===========================>..] - ETA: 0s - loss: 0.0047 - accuracy: 0.9953\n",
            "Epoch 00113: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0049 - accuracy: 0.9951 - val_loss: 0.0205 - val_accuracy: 0.9786\n",
            "Epoch 114/500\n",
            "278/298 [==========================>...] - ETA: 0s - loss: 0.0047 - accuracy: 0.9953\n",
            "Epoch 00114: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0049 - accuracy: 0.9951 - val_loss: 0.0204 - val_accuracy: 0.9794\n",
            "Epoch 115/500\n",
            "277/298 [==========================>...] - ETA: 0s - loss: 0.0049 - accuracy: 0.9950\n",
            "Epoch 00115: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0049 - accuracy: 0.9951 - val_loss: 0.0206 - val_accuracy: 0.9786\n",
            "Epoch 116/500\n",
            "295/298 [============================>.] - ETA: 0s - loss: 0.0050 - accuracy: 0.9950\n",
            "Epoch 00116: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0049 - accuracy: 0.9951 - val_loss: 0.0203 - val_accuracy: 0.9794\n",
            "Epoch 117/500\n",
            "292/298 [============================>.] - ETA: 0s - loss: 0.0050 - accuracy: 0.9950\n",
            "Epoch 00117: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0049 - accuracy: 0.9951 - val_loss: 0.0208 - val_accuracy: 0.9786\n",
            "Epoch 118/500\n",
            "297/298 [============================>.] - ETA: 0s - loss: 0.0048 - accuracy: 0.9952\n",
            "Epoch 00118: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0049 - accuracy: 0.9951 - val_loss: 0.0210 - val_accuracy: 0.9778\n",
            "Epoch 119/500\n",
            "293/298 [============================>.] - ETA: 0s - loss: 0.0057 - accuracy: 0.9939\n",
            "Epoch 00119: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0059 - accuracy: 0.9937 - val_loss: 0.0222 - val_accuracy: 0.9778\n",
            "Epoch 120/500\n",
            "277/298 [==========================>...] - ETA: 0s - loss: 0.0055 - accuracy: 0.9941\n",
            "Epoch 00120: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0053 - accuracy: 0.9943 - val_loss: 0.0186 - val_accuracy: 0.9810\n",
            "Epoch 121/500\n",
            "279/298 [===========================>..] - ETA: 0s - loss: 0.0049 - accuracy: 0.9951\n",
            "Epoch 00121: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0048 - accuracy: 0.9952 - val_loss: 0.0214 - val_accuracy: 0.9778\n",
            "Epoch 122/500\n",
            "285/298 [===========================>..] - ETA: 0s - loss: 0.0049 - accuracy: 0.9951\n",
            "Epoch 00122: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0048 - accuracy: 0.9952 - val_loss: 0.0206 - val_accuracy: 0.9786\n",
            "Epoch 123/500\n",
            "294/298 [============================>.] - ETA: 0s - loss: 0.0049 - accuracy: 0.9951\n",
            "Epoch 00123: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0048 - accuracy: 0.9952 - val_loss: 0.0204 - val_accuracy: 0.9786\n",
            "Epoch 124/500\n",
            "296/298 [============================>.] - ETA: 0s - loss: 0.0047 - accuracy: 0.9952\n",
            "Epoch 00124: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0048 - accuracy: 0.9952 - val_loss: 0.0210 - val_accuracy: 0.9786\n",
            "Epoch 125/500\n",
            "286/298 [===========================>..] - ETA: 0s - loss: 0.0048 - accuracy: 0.9952\n",
            "Epoch 00125: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0048 - accuracy: 0.9952 - val_loss: 0.0205 - val_accuracy: 0.9786\n",
            "Epoch 126/500\n",
            "296/298 [============================>.] - ETA: 0s - loss: 0.0048 - accuracy: 0.9951\n",
            "Epoch 00126: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0048 - accuracy: 0.9952 - val_loss: 0.0210 - val_accuracy: 0.9786\n",
            "Epoch 127/500\n",
            "289/298 [============================>.] - ETA: 0s - loss: 0.0048 - accuracy: 0.9951\n",
            "Epoch 00127: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0048 - accuracy: 0.9952 - val_loss: 0.0202 - val_accuracy: 0.9794\n",
            "Epoch 128/500\n",
            "276/298 [==========================>...] - ETA: 0s - loss: 0.0048 - accuracy: 0.9951\n",
            "Epoch 00128: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0048 - accuracy: 0.9952 - val_loss: 0.0201 - val_accuracy: 0.9794\n",
            "Epoch 129/500\n",
            "293/298 [============================>.] - ETA: 0s - loss: 0.0048 - accuracy: 0.9952\n",
            "Epoch 00129: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0048 - accuracy: 0.9952 - val_loss: 0.0201 - val_accuracy: 0.9794\n",
            "Epoch 130/500\n",
            "286/298 [===========================>..] - ETA: 0s - loss: 0.0050 - accuracy: 0.9950\n",
            "Epoch 00130: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0048 - accuracy: 0.9952 - val_loss: 0.0202 - val_accuracy: 0.9786\n",
            "Epoch 131/500\n",
            "293/298 [============================>.] - ETA: 0s - loss: 0.0049 - accuracy: 0.9951\n",
            "Epoch 00131: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0048 - accuracy: 0.9952 - val_loss: 0.0205 - val_accuracy: 0.9778\n",
            "Epoch 132/500\n",
            "284/298 [===========================>..] - ETA: 0s - loss: 0.0062 - accuracy: 0.9933\n",
            "Epoch 00132: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0064 - accuracy: 0.9932 - val_loss: 0.0230 - val_accuracy: 0.9762\n",
            "Epoch 133/500\n",
            "283/298 [===========================>..] - ETA: 0s - loss: 0.0050 - accuracy: 0.9947\n",
            "Epoch 00133: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0048 - accuracy: 0.9949 - val_loss: 0.0204 - val_accuracy: 0.9794\n",
            "Epoch 134/500\n",
            "291/298 [============================>.] - ETA: 0s - loss: 0.0045 - accuracy: 0.9955\n",
            "Epoch 00134: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0045 - accuracy: 0.9955 - val_loss: 0.0204 - val_accuracy: 0.9794\n",
            "Epoch 135/500\n",
            "283/298 [===========================>..] - ETA: 0s - loss: 0.0044 - accuracy: 0.9956\n",
            "Epoch 00135: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0045 - accuracy: 0.9955 - val_loss: 0.0199 - val_accuracy: 0.9794\n",
            "Epoch 136/500\n",
            "284/298 [===========================>..] - ETA: 0s - loss: 0.0044 - accuracy: 0.9956\n",
            "Epoch 00136: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0045 - accuracy: 0.9955 - val_loss: 0.0200 - val_accuracy: 0.9794\n",
            "Epoch 137/500\n",
            "295/298 [============================>.] - ETA: 0s - loss: 0.0045 - accuracy: 0.9954\n",
            "Epoch 00137: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0045 - accuracy: 0.9955 - val_loss: 0.0199 - val_accuracy: 0.9794\n",
            "Epoch 138/500\n",
            "273/298 [==========================>...] - ETA: 0s - loss: 0.0044 - accuracy: 0.9955\n",
            "Epoch 00138: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0045 - accuracy: 0.9955 - val_loss: 0.0203 - val_accuracy: 0.9794\n",
            "Epoch 139/500\n",
            "277/298 [==========================>...] - ETA: 0s - loss: 0.0044 - accuracy: 0.9956\n",
            "Epoch 00139: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0045 - accuracy: 0.9955 - val_loss: 0.0199 - val_accuracy: 0.9794\n",
            "Epoch 140/500\n",
            "290/298 [============================>.] - ETA: 0s - loss: 0.0045 - accuracy: 0.9955\n",
            "Epoch 00140: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0045 - accuracy: 0.9955 - val_loss: 0.0201 - val_accuracy: 0.9794\n",
            "Epoch 141/500\n",
            "288/298 [===========================>..] - ETA: 0s - loss: 0.0044 - accuracy: 0.9956\n",
            "Epoch 00141: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0045 - accuracy: 0.9955 - val_loss: 0.0204 - val_accuracy: 0.9794\n",
            "Epoch 142/500\n",
            "297/298 [============================>.] - ETA: 0s - loss: 0.0044 - accuracy: 0.9956\n",
            "Epoch 00142: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0045 - accuracy: 0.9955 - val_loss: 0.0201 - val_accuracy: 0.9794\n",
            "Epoch 143/500\n",
            "277/298 [==========================>...] - ETA: 0s - loss: 0.0045 - accuracy: 0.9955\n",
            "Epoch 00143: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0045 - accuracy: 0.9955 - val_loss: 0.0200 - val_accuracy: 0.9794\n",
            "Epoch 144/500\n",
            "286/298 [===========================>..] - ETA: 0s - loss: 0.0047 - accuracy: 0.9953\n",
            "Epoch 00144: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0045 - accuracy: 0.9955 - val_loss: 0.0200 - val_accuracy: 0.9794\n",
            "Epoch 145/500\n",
            "297/298 [============================>.] - ETA: 0s - loss: 0.0045 - accuracy: 0.9955\n",
            "Epoch 00145: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0045 - accuracy: 0.9955 - val_loss: 0.0200 - val_accuracy: 0.9794\n",
            "Epoch 146/500\n",
            "283/298 [===========================>..] - ETA: 0s - loss: 0.0041 - accuracy: 0.9959\n",
            "Epoch 00146: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0045 - accuracy: 0.9955 - val_loss: 0.0200 - val_accuracy: 0.9794\n",
            "Epoch 147/500\n",
            "295/298 [============================>.] - ETA: 0s - loss: 0.0045 - accuracy: 0.9954\n",
            "Epoch 00147: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0045 - accuracy: 0.9955 - val_loss: 0.0200 - val_accuracy: 0.9794\n",
            "Epoch 148/500\n",
            "291/298 [============================>.] - ETA: 0s - loss: 0.0045 - accuracy: 0.9955\n",
            "Epoch 00148: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0045 - accuracy: 0.9955 - val_loss: 0.0196 - val_accuracy: 0.9794\n",
            "Epoch 149/500\n",
            "281/298 [===========================>..] - ETA: 0s - loss: 0.0047 - accuracy: 0.9953\n",
            "Epoch 00149: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0045 - accuracy: 0.9955 - val_loss: 0.0199 - val_accuracy: 0.9794\n",
            "Epoch 150/500\n",
            "282/298 [===========================>..] - ETA: 0s - loss: 0.0044 - accuracy: 0.9956\n",
            "Epoch 00150: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0045 - accuracy: 0.9955 - val_loss: 0.0201 - val_accuracy: 0.9794\n",
            "Epoch 151/500\n",
            "291/298 [============================>.] - ETA: 0s - loss: 0.0046 - accuracy: 0.9954\n",
            "Epoch 00151: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0045 - accuracy: 0.9955 - val_loss: 0.0199 - val_accuracy: 0.9794\n",
            "Epoch 152/500\n",
            "282/298 [===========================>..] - ETA: 0s - loss: 0.0044 - accuracy: 0.9956\n",
            "Epoch 00152: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0045 - accuracy: 0.9955 - val_loss: 0.0202 - val_accuracy: 0.9794\n",
            "Epoch 153/500\n",
            "293/298 [============================>.] - ETA: 0s - loss: 0.0045 - accuracy: 0.9955\n",
            "Epoch 00153: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0045 - accuracy: 0.9955 - val_loss: 0.0197 - val_accuracy: 0.9794\n",
            "Epoch 154/500\n",
            "274/298 [==========================>...] - ETA: 0s - loss: 0.0044 - accuracy: 0.9956\n",
            "Epoch 00154: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0045 - accuracy: 0.9955 - val_loss: 0.0206 - val_accuracy: 0.9786\n",
            "Epoch 155/500\n",
            "294/298 [============================>.] - ETA: 0s - loss: 0.0046 - accuracy: 0.9954\n",
            "Epoch 00155: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0045 - accuracy: 0.9955 - val_loss: 0.0193 - val_accuracy: 0.9794\n",
            "Epoch 156/500\n",
            "280/298 [===========================>..] - ETA: 0s - loss: 0.0044 - accuracy: 0.9955\n",
            "Epoch 00156: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0045 - accuracy: 0.9955 - val_loss: 0.0194 - val_accuracy: 0.9794\n",
            "Epoch 157/500\n",
            "292/298 [============================>.] - ETA: 0s - loss: 0.0045 - accuracy: 0.9955\n",
            "Epoch 00157: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0045 - accuracy: 0.9955 - val_loss: 0.0203 - val_accuracy: 0.9786\n",
            "Epoch 158/500\n",
            "287/298 [===========================>..] - ETA: 0s - loss: 0.0051 - accuracy: 0.9948\n",
            "Epoch 00158: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0050 - accuracy: 0.9949 - val_loss: 0.0184 - val_accuracy: 0.9817\n",
            "Epoch 159/500\n",
            "297/298 [============================>.] - ETA: 0s - loss: 0.0047 - accuracy: 0.9954\n",
            "Epoch 00159: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0047 - accuracy: 0.9954 - val_loss: 0.0202 - val_accuracy: 0.9770\n",
            "Epoch 160/500\n",
            "296/298 [============================>.] - ETA: 0s - loss: 0.0043 - accuracy: 0.9957\n",
            "Epoch 00160: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0045 - accuracy: 0.9955 - val_loss: 0.0210 - val_accuracy: 0.9762\n",
            "Epoch 161/500\n",
            "289/298 [============================>.] - ETA: 0s - loss: 0.0046 - accuracy: 0.9954\n",
            "Epoch 00161: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0045 - accuracy: 0.9955 - val_loss: 0.0212 - val_accuracy: 0.9746\n",
            "Epoch 162/500\n",
            "291/298 [============================>.] - ETA: 0s - loss: 0.0046 - accuracy: 0.9954\n",
            "Epoch 00162: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0045 - accuracy: 0.9955 - val_loss: 0.0205 - val_accuracy: 0.9778\n",
            "Epoch 163/500\n",
            "286/298 [===========================>..] - ETA: 0s - loss: 0.0044 - accuracy: 0.9956\n",
            "Epoch 00163: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0045 - accuracy: 0.9955 - val_loss: 0.0208 - val_accuracy: 0.9762\n",
            "Epoch 164/500\n",
            "294/298 [============================>.] - ETA: 0s - loss: 0.0046 - accuracy: 0.9954\n",
            "Epoch 00164: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0045 - accuracy: 0.9955 - val_loss: 0.0205 - val_accuracy: 0.9778\n",
            "Epoch 165/500\n",
            "285/298 [===========================>..] - ETA: 0s - loss: 0.0045 - accuracy: 0.9955\n",
            "Epoch 00165: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0045 - accuracy: 0.9955 - val_loss: 0.0207 - val_accuracy: 0.9778\n",
            "Epoch 166/500\n",
            "290/298 [============================>.] - ETA: 0s - loss: 0.0043 - accuracy: 0.9957\n",
            "Epoch 00166: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0045 - accuracy: 0.9955 - val_loss: 0.0209 - val_accuracy: 0.9762\n",
            "Epoch 167/500\n",
            "294/298 [============================>.] - ETA: 0s - loss: 0.0046 - accuracy: 0.9954\n",
            "Epoch 00167: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0045 - accuracy: 0.9955 - val_loss: 0.0203 - val_accuracy: 0.9786\n",
            "Epoch 168/500\n",
            "279/298 [===========================>..] - ETA: 0s - loss: 0.0046 - accuracy: 0.9954\n",
            "Epoch 00168: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0045 - accuracy: 0.9955 - val_loss: 0.0205 - val_accuracy: 0.9778\n",
            "Epoch 169/500\n",
            "290/298 [============================>.] - ETA: 0s - loss: 0.0046 - accuracy: 0.9954\n",
            "Epoch 00169: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0045 - accuracy: 0.9955 - val_loss: 0.0202 - val_accuracy: 0.9778\n",
            "Epoch 170/500\n",
            "289/298 [============================>.] - ETA: 0s - loss: 0.0046 - accuracy: 0.9954\n",
            "Epoch 00170: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0045 - accuracy: 0.9955 - val_loss: 0.0201 - val_accuracy: 0.9778\n",
            "Epoch 171/500\n",
            "290/298 [============================>.] - ETA: 0s - loss: 0.0046 - accuracy: 0.9954\n",
            "Epoch 00171: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0045 - accuracy: 0.9955 - val_loss: 0.0200 - val_accuracy: 0.9778\n",
            "Epoch 172/500\n",
            "297/298 [============================>.] - ETA: 0s - loss: 0.0044 - accuracy: 0.9956\n",
            "Epoch 00172: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0045 - accuracy: 0.9955 - val_loss: 0.0199 - val_accuracy: 0.9786\n",
            "Epoch 173/500\n",
            "282/298 [===========================>..] - ETA: 0s - loss: 0.0046 - accuracy: 0.9953\n",
            "Epoch 00173: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0045 - accuracy: 0.9955 - val_loss: 0.0199 - val_accuracy: 0.9794\n",
            "Epoch 174/500\n",
            "296/298 [============================>.] - ETA: 0s - loss: 0.0044 - accuracy: 0.9956\n",
            "Epoch 00174: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0045 - accuracy: 0.9955 - val_loss: 0.0198 - val_accuracy: 0.9794\n",
            "Epoch 175/500\n",
            "287/298 [===========================>..] - ETA: 0s - loss: 0.0046 - accuracy: 0.9954\n",
            "Epoch 00175: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0045 - accuracy: 0.9955 - val_loss: 0.0195 - val_accuracy: 0.9786\n",
            "Epoch 176/500\n",
            "278/298 [==========================>...] - ETA: 0s - loss: 0.0046 - accuracy: 0.9954\n",
            "Epoch 00176: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0045 - accuracy: 0.9955 - val_loss: 0.0206 - val_accuracy: 0.9778\n",
            "Epoch 177/500\n",
            "298/298 [==============================] - ETA: 0s - loss: 0.0052 - accuracy: 0.9944\n",
            "Epoch 00177: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0052 - accuracy: 0.9944 - val_loss: 0.0256 - val_accuracy: 0.9730\n",
            "Epoch 178/500\n",
            "282/298 [===========================>..] - ETA: 0s - loss: 0.0053 - accuracy: 0.9942\n",
            "Epoch 00178: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0051 - accuracy: 0.9944 - val_loss: 0.0180 - val_accuracy: 0.9817\n",
            "Epoch 179/500\n",
            "298/298 [==============================] - ETA: 0s - loss: 0.0046 - accuracy: 0.9953\n",
            "Epoch 00179: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0046 - accuracy: 0.9953 - val_loss: 0.0197 - val_accuracy: 0.9802\n",
            "Epoch 180/500\n",
            "286/298 [===========================>..] - ETA: 0s - loss: 0.0045 - accuracy: 0.9954\n",
            "Epoch 00180: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0044 - accuracy: 0.9956 - val_loss: 0.0192 - val_accuracy: 0.9802\n",
            "Epoch 181/500\n",
            "282/298 [===========================>..] - ETA: 0s - loss: 0.0041 - accuracy: 0.9959\n",
            "Epoch 00181: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0043 - accuracy: 0.9957 - val_loss: 0.0202 - val_accuracy: 0.9786\n",
            "Epoch 182/500\n",
            "296/298 [============================>.] - ETA: 0s - loss: 0.0043 - accuracy: 0.9957\n",
            "Epoch 00182: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0043 - accuracy: 0.9957 - val_loss: 0.0193 - val_accuracy: 0.9794\n",
            "Epoch 183/500\n",
            "276/298 [==========================>...] - ETA: 0s - loss: 0.0044 - accuracy: 0.9956\n",
            "Epoch 00183: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0043 - accuracy: 0.9957 - val_loss: 0.0193 - val_accuracy: 0.9794\n",
            "Epoch 184/500\n",
            "282/298 [===========================>..] - ETA: 0s - loss: 0.0045 - accuracy: 0.9955\n",
            "Epoch 00184: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0043 - accuracy: 0.9957 - val_loss: 0.0192 - val_accuracy: 0.9794\n",
            "Epoch 185/500\n",
            "289/298 [============================>.] - ETA: 0s - loss: 0.0040 - accuracy: 0.9960\n",
            "Epoch 00185: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0043 - accuracy: 0.9957 - val_loss: 0.0192 - val_accuracy: 0.9794\n",
            "Epoch 186/500\n",
            "285/298 [===========================>..] - ETA: 0s - loss: 0.0040 - accuracy: 0.9959\n",
            "Epoch 00186: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0043 - accuracy: 0.9957 - val_loss: 0.0192 - val_accuracy: 0.9794\n",
            "Epoch 187/500\n",
            "295/298 [============================>.] - ETA: 0s - loss: 0.0043 - accuracy: 0.9957\n",
            "Epoch 00187: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0043 - accuracy: 0.9957 - val_loss: 0.0189 - val_accuracy: 0.9794\n",
            "Epoch 188/500\n",
            "285/298 [===========================>..] - ETA: 0s - loss: 0.0045 - accuracy: 0.9955\n",
            "Epoch 00188: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0043 - accuracy: 0.9957 - val_loss: 0.0193 - val_accuracy: 0.9786\n",
            "Epoch 189/500\n",
            "292/298 [============================>.] - ETA: 0s - loss: 0.0043 - accuracy: 0.9957\n",
            "Epoch 00189: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0043 - accuracy: 0.9957 - val_loss: 0.0191 - val_accuracy: 0.9786\n",
            "Epoch 190/500\n",
            "297/298 [============================>.] - ETA: 0s - loss: 0.0043 - accuracy: 0.9957\n",
            "Epoch 00190: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0043 - accuracy: 0.9957 - val_loss: 0.0192 - val_accuracy: 0.9786\n",
            "Epoch 191/500\n",
            "297/298 [============================>.] - ETA: 0s - loss: 0.0043 - accuracy: 0.9957\n",
            "Epoch 00191: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0043 - accuracy: 0.9957 - val_loss: 0.0191 - val_accuracy: 0.9786\n",
            "Epoch 192/500\n",
            "289/298 [============================>.] - ETA: 0s - loss: 0.0043 - accuracy: 0.9957\n",
            "Epoch 00192: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0043 - accuracy: 0.9957 - val_loss: 0.0190 - val_accuracy: 0.9786\n",
            "Epoch 193/500\n",
            "290/298 [============================>.] - ETA: 0s - loss: 0.0044 - accuracy: 0.9956\n",
            "Epoch 00193: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0043 - accuracy: 0.9957 - val_loss: 0.0194 - val_accuracy: 0.9786\n",
            "Epoch 194/500\n",
            "285/298 [===========================>..] - ETA: 0s - loss: 0.0042 - accuracy: 0.9958\n",
            "Epoch 00194: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0043 - accuracy: 0.9957 - val_loss: 0.0192 - val_accuracy: 0.9786\n",
            "Epoch 195/500\n",
            "281/298 [===========================>..] - ETA: 0s - loss: 0.0043 - accuracy: 0.9957\n",
            "Epoch 00195: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0043 - accuracy: 0.9957 - val_loss: 0.0189 - val_accuracy: 0.9794\n",
            "Epoch 196/500\n",
            "285/298 [===========================>..] - ETA: 0s - loss: 0.0043 - accuracy: 0.9957\n",
            "Epoch 00196: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0043 - accuracy: 0.9957 - val_loss: 0.0196 - val_accuracy: 0.9786\n",
            "Epoch 197/500\n",
            "275/298 [==========================>...] - ETA: 0s - loss: 0.0043 - accuracy: 0.9957\n",
            "Epoch 00197: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0043 - accuracy: 0.9957 - val_loss: 0.0191 - val_accuracy: 0.9794\n",
            "Epoch 198/500\n",
            "278/298 [==========================>...] - ETA: 0s - loss: 0.0043 - accuracy: 0.9957\n",
            "Epoch 00198: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0043 - accuracy: 0.9957 - val_loss: 0.0192 - val_accuracy: 0.9794\n",
            "Epoch 199/500\n",
            "286/298 [===========================>..] - ETA: 0s - loss: 0.0044 - accuracy: 0.9956\n",
            "Epoch 00199: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0043 - accuracy: 0.9957 - val_loss: 0.0186 - val_accuracy: 0.9794\n",
            "Epoch 200/500\n",
            "296/298 [============================>.] - ETA: 0s - loss: 0.0043 - accuracy: 0.9957\n",
            "Epoch 00200: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0043 - accuracy: 0.9957 - val_loss: 0.0195 - val_accuracy: 0.9794\n",
            "Epoch 201/500\n",
            "294/298 [============================>.] - ETA: 0s - loss: 0.0043 - accuracy: 0.9956\n",
            "Epoch 00201: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0043 - accuracy: 0.9957 - val_loss: 0.0190 - val_accuracy: 0.9794\n",
            "Epoch 202/500\n",
            "275/298 [==========================>...] - ETA: 0s - loss: 0.0043 - accuracy: 0.9957\n",
            "Epoch 00202: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0043 - accuracy: 0.9957 - val_loss: 0.0189 - val_accuracy: 0.9794\n",
            "Epoch 203/500\n",
            "277/298 [==========================>...] - ETA: 0s - loss: 0.0042 - accuracy: 0.9958\n",
            "Epoch 00203: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0043 - accuracy: 0.9957 - val_loss: 0.0188 - val_accuracy: 0.9794\n",
            "Epoch 204/500\n",
            "279/298 [===========================>..] - ETA: 0s - loss: 0.0045 - accuracy: 0.9955\n",
            "Epoch 00204: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0043 - accuracy: 0.9957 - val_loss: 0.0197 - val_accuracy: 0.9794\n",
            "Epoch 205/500\n",
            "280/298 [===========================>..] - ETA: 0s - loss: 0.0043 - accuracy: 0.9956\n",
            "Epoch 00205: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0043 - accuracy: 0.9957 - val_loss: 0.0205 - val_accuracy: 0.9786\n",
            "Epoch 206/500\n",
            "296/298 [============================>.] - ETA: 0s - loss: 0.0042 - accuracy: 0.9958\n",
            "Epoch 00206: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0043 - accuracy: 0.9957 - val_loss: 0.0194 - val_accuracy: 0.9794\n",
            "Epoch 207/500\n",
            "278/298 [==========================>...] - ETA: 0s - loss: 0.0038 - accuracy: 0.9962\n",
            "Epoch 00207: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0043 - accuracy: 0.9957 - val_loss: 0.0204 - val_accuracy: 0.9786\n",
            "Epoch 208/500\n",
            "295/298 [============================>.] - ETA: 0s - loss: 0.0053 - accuracy: 0.9944\n",
            "Epoch 00208: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0053 - accuracy: 0.9944 - val_loss: 0.0166 - val_accuracy: 0.9817\n",
            "Epoch 209/500\n",
            "284/298 [===========================>..] - ETA: 0s - loss: 0.0047 - accuracy: 0.9953\n",
            "Epoch 00209: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0046 - accuracy: 0.9954 - val_loss: 0.0223 - val_accuracy: 0.9754\n",
            "Epoch 210/500\n",
            "287/298 [===========================>..] - ETA: 0s - loss: 0.0044 - accuracy: 0.9955\n",
            "Epoch 00210: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0043 - accuracy: 0.9957 - val_loss: 0.0214 - val_accuracy: 0.9762\n",
            "Epoch 211/500\n",
            "294/298 [============================>.] - ETA: 0s - loss: 0.0043 - accuracy: 0.9956\n",
            "Epoch 00211: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0043 - accuracy: 0.9957 - val_loss: 0.0207 - val_accuracy: 0.9778\n",
            "Epoch 212/500\n",
            "292/298 [============================>.] - ETA: 0s - loss: 0.0044 - accuracy: 0.9956\n",
            "Epoch 00212: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0043 - accuracy: 0.9957 - val_loss: 0.0204 - val_accuracy: 0.9778\n",
            "Epoch 213/500\n",
            "284/298 [===========================>..] - ETA: 0s - loss: 0.0043 - accuracy: 0.9957\n",
            "Epoch 00213: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0043 - accuracy: 0.9957 - val_loss: 0.0204 - val_accuracy: 0.9778\n",
            "Epoch 214/500\n",
            "287/298 [===========================>..] - ETA: 0s - loss: 0.0040 - accuracy: 0.9960\n",
            "Epoch 00214: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0043 - accuracy: 0.9957 - val_loss: 0.0201 - val_accuracy: 0.9778\n",
            "Epoch 215/500\n",
            "291/298 [============================>.] - ETA: 0s - loss: 0.0044 - accuracy: 0.9956\n",
            "Epoch 00215: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0043 - accuracy: 0.9957 - val_loss: 0.0200 - val_accuracy: 0.9778\n",
            "Epoch 216/500\n",
            "288/298 [===========================>..] - ETA: 0s - loss: 0.0043 - accuracy: 0.9957\n",
            "Epoch 00216: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0043 - accuracy: 0.9957 - val_loss: 0.0200 - val_accuracy: 0.9778\n",
            "Epoch 217/500\n",
            "297/298 [============================>.] - ETA: 0s - loss: 0.0043 - accuracy: 0.9957\n",
            "Epoch 00217: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0043 - accuracy: 0.9957 - val_loss: 0.0199 - val_accuracy: 0.9778\n",
            "Epoch 218/500\n",
            "292/298 [============================>.] - ETA: 0s - loss: 0.0043 - accuracy: 0.9957\n",
            "Epoch 00218: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0043 - accuracy: 0.9957 - val_loss: 0.0198 - val_accuracy: 0.9786\n",
            "Epoch 219/500\n",
            "283/298 [===========================>..] - ETA: 0s - loss: 0.0044 - accuracy: 0.9956\n",
            "Epoch 00219: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0043 - accuracy: 0.9957 - val_loss: 0.0196 - val_accuracy: 0.9794\n",
            "Epoch 220/500\n",
            "282/298 [===========================>..] - ETA: 0s - loss: 0.0043 - accuracy: 0.9957\n",
            "Epoch 00220: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0043 - accuracy: 0.9957 - val_loss: 0.0196 - val_accuracy: 0.9802\n",
            "Epoch 221/500\n",
            "278/298 [==========================>...] - ETA: 0s - loss: 0.0044 - accuracy: 0.9956\n",
            "Epoch 00221: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0043 - accuracy: 0.9957 - val_loss: 0.0192 - val_accuracy: 0.9810\n",
            "Epoch 222/500\n",
            "292/298 [============================>.] - ETA: 0s - loss: 0.0044 - accuracy: 0.9956\n",
            "Epoch 00222: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0043 - accuracy: 0.9957 - val_loss: 0.0191 - val_accuracy: 0.9810\n",
            "Epoch 223/500\n",
            "296/298 [============================>.] - ETA: 0s - loss: 0.0043 - accuracy: 0.9957\n",
            "Epoch 00223: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0043 - accuracy: 0.9957 - val_loss: 0.0193 - val_accuracy: 0.9802\n",
            "Epoch 224/500\n",
            "292/298 [============================>.] - ETA: 0s - loss: 0.0044 - accuracy: 0.9956\n",
            "Epoch 00224: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0043 - accuracy: 0.9957 - val_loss: 0.0190 - val_accuracy: 0.9810\n",
            "Epoch 225/500\n",
            "287/298 [===========================>..] - ETA: 0s - loss: 0.0043 - accuracy: 0.9956\n",
            "Epoch 00225: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0043 - accuracy: 0.9957 - val_loss: 0.0192 - val_accuracy: 0.9802\n",
            "Epoch 226/500\n",
            "298/298 [==============================] - ETA: 0s - loss: 0.0043 - accuracy: 0.9957\n",
            "Epoch 00226: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0043 - accuracy: 0.9957 - val_loss: 0.0186 - val_accuracy: 0.9810\n",
            "Epoch 227/500\n",
            "289/298 [============================>.] - ETA: 0s - loss: 0.0042 - accuracy: 0.9958\n",
            "Epoch 00227: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0043 - accuracy: 0.9957 - val_loss: 0.0186 - val_accuracy: 0.9802\n",
            "Epoch 228/500\n",
            "291/298 [============================>.] - ETA: 0s - loss: 0.0044 - accuracy: 0.9956\n",
            "Epoch 00228: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0043 - accuracy: 0.9957 - val_loss: 0.0179 - val_accuracy: 0.9802\n",
            "Epoch 229/500\n",
            "284/298 [===========================>..] - ETA: 0s - loss: 0.0044 - accuracy: 0.9956\n",
            "Epoch 00229: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0043 - accuracy: 0.9957 - val_loss: 0.0193 - val_accuracy: 0.9786\n",
            "Epoch 230/500\n",
            "293/298 [============================>.] - ETA: 0s - loss: 0.0049 - accuracy: 0.9950\n",
            "Epoch 00230: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.0048 - accuracy: 0.9951 - val_loss: 0.0221 - val_accuracy: 0.9754\n",
            "Epoch 231/500\n",
            "293/298 [============================>.] - ETA: 0s - loss: 0.0043 - accuracy: 0.9956\n",
            "Epoch 00231: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0045 - accuracy: 0.9955 - val_loss: 0.0200 - val_accuracy: 0.9794\n",
            "Epoch 232/500\n",
            "291/298 [============================>.] - ETA: 0s - loss: 0.0042 - accuracy: 0.9958\n",
            "Epoch 00232: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0043 - accuracy: 0.9957 - val_loss: 0.0197 - val_accuracy: 0.9802\n",
            "Epoch 233/500\n",
            "297/298 [============================>.] - ETA: 0s - loss: 0.0042 - accuracy: 0.9958\n",
            "Epoch 00233: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.0043 - accuracy: 0.9957 - val_loss: 0.0204 - val_accuracy: 0.9770\n",
            "Epoch 234/500\n",
            "286/298 [===========================>..] - ETA: 0s - loss: 0.0041 - accuracy: 0.9958\n",
            "Epoch 00234: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0043 - accuracy: 0.9957 - val_loss: 0.0209 - val_accuracy: 0.9778\n",
            "Epoch 235/500\n",
            "280/298 [===========================>..] - ETA: 0s - loss: 0.0042 - accuracy: 0.9958\n",
            "Epoch 00235: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0043 - accuracy: 0.9957 - val_loss: 0.0208 - val_accuracy: 0.9778\n",
            "Epoch 236/500\n",
            "297/298 [============================>.] - ETA: 0s - loss: 0.0043 - accuracy: 0.9957\n",
            "Epoch 00236: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0043 - accuracy: 0.9957 - val_loss: 0.0207 - val_accuracy: 0.9778\n",
            "Epoch 237/500\n",
            "293/298 [============================>.] - ETA: 0s - loss: 0.0044 - accuracy: 0.9956\n",
            "Epoch 00237: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0043 - accuracy: 0.9957 - val_loss: 0.0204 - val_accuracy: 0.9778\n",
            "Epoch 238/500\n",
            "288/298 [===========================>..] - ETA: 0s - loss: 0.0042 - accuracy: 0.9958\n",
            "Epoch 00238: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0043 - accuracy: 0.9957 - val_loss: 0.0204 - val_accuracy: 0.9778\n",
            "Epoch 239/500\n",
            "284/298 [===========================>..] - ETA: 0s - loss: 0.0044 - accuracy: 0.9956\n",
            "Epoch 00239: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0043 - accuracy: 0.9957 - val_loss: 0.0199 - val_accuracy: 0.9802\n",
            "Epoch 240/500\n",
            "285/298 [===========================>..] - ETA: 0s - loss: 0.0044 - accuracy: 0.9956\n",
            "Epoch 00240: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0043 - accuracy: 0.9957 - val_loss: 0.0196 - val_accuracy: 0.9802\n",
            "Epoch 241/500\n",
            "283/298 [===========================>..] - ETA: 0s - loss: 0.0041 - accuracy: 0.9959\n",
            "Epoch 00241: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0043 - accuracy: 0.9957 - val_loss: 0.0182 - val_accuracy: 0.9817\n",
            "Epoch 242/500\n",
            "291/298 [============================>.] - ETA: 0s - loss: 0.0047 - accuracy: 0.9953\n",
            "Epoch 00242: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0046 - accuracy: 0.9953 - val_loss: 0.0211 - val_accuracy: 0.9786\n",
            "Epoch 243/500\n",
            "292/298 [============================>.] - ETA: 0s - loss: 0.0052 - accuracy: 0.9948\n",
            "Epoch 00243: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0051 - accuracy: 0.9949 - val_loss: 0.0197 - val_accuracy: 0.9794\n",
            "Epoch 244/500\n",
            "282/298 [===========================>..] - ETA: 0s - loss: 0.0045 - accuracy: 0.9953\n",
            "Epoch 00244: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0045 - accuracy: 0.9954 - val_loss: 0.0224 - val_accuracy: 0.9762\n",
            "Epoch 245/500\n",
            "295/298 [============================>.] - ETA: 0s - loss: 0.0043 - accuracy: 0.9956\n",
            "Epoch 00245: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0043 - accuracy: 0.9956 - val_loss: 0.0170 - val_accuracy: 0.9810\n",
            "Epoch 246/500\n",
            "298/298 [==============================] - ETA: 0s - loss: 0.0043 - accuracy: 0.9957\n",
            "Epoch 00246: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0043 - accuracy: 0.9957 - val_loss: 0.0198 - val_accuracy: 0.9786\n",
            "Epoch 247/500\n",
            "285/298 [===========================>..] - ETA: 0s - loss: 0.0039 - accuracy: 0.9961\n",
            "Epoch 00247: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0042 - accuracy: 0.9958 - val_loss: 0.0198 - val_accuracy: 0.9786\n",
            "Epoch 248/500\n",
            "287/298 [===========================>..] - ETA: 0s - loss: 0.0042 - accuracy: 0.9958\n",
            "Epoch 00248: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0042 - accuracy: 0.9958 - val_loss: 0.0198 - val_accuracy: 0.9786\n",
            "Epoch 249/500\n",
            "293/298 [============================>.] - ETA: 0s - loss: 0.0041 - accuracy: 0.9958\n",
            "Epoch 00249: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0042 - accuracy: 0.9958 - val_loss: 0.0197 - val_accuracy: 0.9770\n",
            "Epoch 250/500\n",
            "292/298 [============================>.] - ETA: 0s - loss: 0.0043 - accuracy: 0.9957\n",
            "Epoch 00250: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0042 - accuracy: 0.9958 - val_loss: 0.0198 - val_accuracy: 0.9770\n",
            "Epoch 251/500\n",
            "278/298 [==========================>...] - ETA: 0s - loss: 0.0043 - accuracy: 0.9957\n",
            "Epoch 00251: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0042 - accuracy: 0.9958 - val_loss: 0.0198 - val_accuracy: 0.9770\n",
            "Epoch 252/500\n",
            "290/298 [============================>.] - ETA: 0s - loss: 0.0043 - accuracy: 0.9957\n",
            "Epoch 00252: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0042 - accuracy: 0.9958 - val_loss: 0.0202 - val_accuracy: 0.9762\n",
            "Epoch 253/500\n",
            "291/298 [============================>.] - ETA: 0s - loss: 0.0043 - accuracy: 0.9957\n",
            "Epoch 00253: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0042 - accuracy: 0.9958 - val_loss: 0.0200 - val_accuracy: 0.9770\n",
            "Epoch 254/500\n",
            "279/298 [===========================>..] - ETA: 0s - loss: 0.0045 - accuracy: 0.9955\n",
            "Epoch 00254: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0042 - accuracy: 0.9958 - val_loss: 0.0199 - val_accuracy: 0.9762\n",
            "Epoch 255/500\n",
            "285/298 [===========================>..] - ETA: 0s - loss: 0.0040 - accuracy: 0.9959\n",
            "Epoch 00255: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0042 - accuracy: 0.9958 - val_loss: 0.0201 - val_accuracy: 0.9770\n",
            "Epoch 256/500\n",
            "297/298 [============================>.] - ETA: 0s - loss: 0.0042 - accuracy: 0.9958\n",
            "Epoch 00256: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0042 - accuracy: 0.9958 - val_loss: 0.0197 - val_accuracy: 0.9778\n",
            "Epoch 257/500\n",
            "280/298 [===========================>..] - ETA: 0s - loss: 0.0043 - accuracy: 0.9956\n",
            "Epoch 00257: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0042 - accuracy: 0.9958 - val_loss: 0.0203 - val_accuracy: 0.9770\n",
            "Epoch 258/500\n",
            "283/298 [===========================>..] - ETA: 0s - loss: 0.0041 - accuracy: 0.9959\n",
            "Epoch 00258: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0042 - accuracy: 0.9958 - val_loss: 0.0197 - val_accuracy: 0.9770\n",
            "Epoch 259/500\n",
            "278/298 [==========================>...] - ETA: 0s - loss: 0.0043 - accuracy: 0.9957\n",
            "Epoch 00259: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0042 - accuracy: 0.9958 - val_loss: 0.0206 - val_accuracy: 0.9762\n",
            "Epoch 260/500\n",
            "276/298 [==========================>...] - ETA: 0s - loss: 0.0039 - accuracy: 0.9960\n",
            "Epoch 00260: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0042 - accuracy: 0.9958 - val_loss: 0.0203 - val_accuracy: 0.9770\n",
            "Epoch 261/500\n",
            "291/298 [============================>.] - ETA: 0s - loss: 0.0042 - accuracy: 0.9958\n",
            "Epoch 00261: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0042 - accuracy: 0.9958 - val_loss: 0.0197 - val_accuracy: 0.9770\n",
            "Epoch 262/500\n",
            "280/298 [===========================>..] - ETA: 0s - loss: 0.0042 - accuracy: 0.9958\n",
            "Epoch 00262: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0042 - accuracy: 0.9958 - val_loss: 0.0206 - val_accuracy: 0.9770\n",
            "Epoch 263/500\n",
            "293/298 [============================>.] - ETA: 0s - loss: 0.0041 - accuracy: 0.9958\n",
            "Epoch 00263: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0042 - accuracy: 0.9958 - val_loss: 0.0201 - val_accuracy: 0.9778\n",
            "Epoch 264/500\n",
            "295/298 [============================>.] - ETA: 0s - loss: 0.0040 - accuracy: 0.9960\n",
            "Epoch 00264: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0042 - accuracy: 0.9958 - val_loss: 0.0204 - val_accuracy: 0.9770\n",
            "Epoch 265/500\n",
            "286/298 [===========================>..] - ETA: 0s - loss: 0.0042 - accuracy: 0.9957\n",
            "Epoch 00265: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0042 - accuracy: 0.9958 - val_loss: 0.0188 - val_accuracy: 0.9786\n",
            "Epoch 266/500\n",
            "282/298 [===========================>..] - ETA: 0s - loss: 0.0042 - accuracy: 0.9958\n",
            "Epoch 00266: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0042 - accuracy: 0.9958 - val_loss: 0.0183 - val_accuracy: 0.9794\n",
            "Epoch 267/500\n",
            "280/298 [===========================>..] - ETA: 0s - loss: 0.0039 - accuracy: 0.9961\n",
            "Epoch 00267: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0042 - accuracy: 0.9958 - val_loss: 0.0197 - val_accuracy: 0.9778\n",
            "Epoch 268/500\n",
            "297/298 [============================>.] - ETA: 0s - loss: 0.0042 - accuracy: 0.9958\n",
            "Epoch 00268: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0042 - accuracy: 0.9958 - val_loss: 0.0198 - val_accuracy: 0.9778\n",
            "Epoch 269/500\n",
            "297/298 [============================>.] - ETA: 0s - loss: 0.0042 - accuracy: 0.9958\n",
            "Epoch 00269: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0042 - accuracy: 0.9958 - val_loss: 0.0199 - val_accuracy: 0.9778\n",
            "Epoch 270/500\n",
            "285/298 [===========================>..] - ETA: 0s - loss: 0.0041 - accuracy: 0.9958\n",
            "Epoch 00270: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0042 - accuracy: 0.9958 - val_loss: 0.0199 - val_accuracy: 0.9778\n",
            "Epoch 271/500\n",
            "279/298 [===========================>..] - ETA: 0s - loss: 0.0043 - accuracy: 0.9956\n",
            "Epoch 00271: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0042 - accuracy: 0.9958 - val_loss: 0.0199 - val_accuracy: 0.9770\n",
            "Epoch 272/500\n",
            "296/298 [============================>.] - ETA: 0s - loss: 0.0045 - accuracy: 0.9955\n",
            "Epoch 00272: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0045 - accuracy: 0.9954 - val_loss: 0.0177 - val_accuracy: 0.9810\n",
            "Epoch 273/500\n",
            "296/298 [============================>.] - ETA: 0s - loss: 0.0048 - accuracy: 0.9947\n",
            "Epoch 00273: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0048 - accuracy: 0.9948 - val_loss: 0.0218 - val_accuracy: 0.9762\n",
            "Epoch 274/500\n",
            "283/298 [===========================>..] - ETA: 0s - loss: 0.0042 - accuracy: 0.9957\n",
            "Epoch 00274: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0042 - accuracy: 0.9957 - val_loss: 0.0202 - val_accuracy: 0.9786\n",
            "Epoch 275/500\n",
            "295/298 [============================>.] - ETA: 0s - loss: 0.0043 - accuracy: 0.9957\n",
            "Epoch 00275: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0042 - accuracy: 0.9957 - val_loss: 0.0273 - val_accuracy: 0.9722\n",
            "Epoch 276/500\n",
            "284/298 [===========================>..] - ETA: 0s - loss: 0.0043 - accuracy: 0.9957\n",
            "Epoch 00276: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0043 - accuracy: 0.9957 - val_loss: 0.0191 - val_accuracy: 0.9794\n",
            "Epoch 277/500\n",
            "291/298 [============================>.] - ETA: 0s - loss: 0.0043 - accuracy: 0.9957\n",
            "Epoch 00277: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0042 - accuracy: 0.9958 - val_loss: 0.0222 - val_accuracy: 0.9770\n",
            "Epoch 278/500\n",
            "286/298 [===========================>..] - ETA: 0s - loss: 0.0040 - accuracy: 0.9960\n",
            "Epoch 00278: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0042 - accuracy: 0.9958 - val_loss: 0.0210 - val_accuracy: 0.9770\n",
            "Epoch 279/500\n",
            "286/298 [===========================>..] - ETA: 0s - loss: 0.0040 - accuracy: 0.9960\n",
            "Epoch 00279: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0042 - accuracy: 0.9958 - val_loss: 0.0211 - val_accuracy: 0.9770\n",
            "Epoch 280/500\n",
            "289/298 [============================>.] - ETA: 0s - loss: 0.0043 - accuracy: 0.9957\n",
            "Epoch 00280: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0042 - accuracy: 0.9958 - val_loss: 0.0211 - val_accuracy: 0.9770\n",
            "Epoch 281/500\n",
            "286/298 [===========================>..] - ETA: 0s - loss: 0.0043 - accuracy: 0.9956\n",
            "Epoch 00281: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0042 - accuracy: 0.9958 - val_loss: 0.0211 - val_accuracy: 0.9770\n",
            "Epoch 282/500\n",
            "295/298 [============================>.] - ETA: 0s - loss: 0.0042 - accuracy: 0.9958\n",
            "Epoch 00282: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0042 - accuracy: 0.9958 - val_loss: 0.0213 - val_accuracy: 0.9770\n",
            "Epoch 283/500\n",
            "292/298 [============================>.] - ETA: 0s - loss: 0.0040 - accuracy: 0.9959\n",
            "Epoch 00283: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0042 - accuracy: 0.9958 - val_loss: 0.0231 - val_accuracy: 0.9762\n",
            "Epoch 284/500\n",
            "279/298 [===========================>..] - ETA: 0s - loss: 0.0042 - accuracy: 0.9957\n",
            "Epoch 00284: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0041 - accuracy: 0.9959 - val_loss: 0.0214 - val_accuracy: 0.9770\n",
            "Epoch 285/500\n",
            "286/298 [===========================>..] - ETA: 0s - loss: 0.0042 - accuracy: 0.9957\n",
            "Epoch 00285: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0041 - accuracy: 0.9959 - val_loss: 0.0216 - val_accuracy: 0.9770\n",
            "Epoch 286/500\n",
            "286/298 [===========================>..] - ETA: 0s - loss: 0.0041 - accuracy: 0.9958\n",
            "Epoch 00286: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0041 - accuracy: 0.9959 - val_loss: 0.0215 - val_accuracy: 0.9770\n",
            "Epoch 287/500\n",
            "282/298 [===========================>..] - ETA: 0s - loss: 0.0040 - accuracy: 0.9960\n",
            "Epoch 00287: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0041 - accuracy: 0.9959 - val_loss: 0.0214 - val_accuracy: 0.9770\n",
            "Epoch 288/500\n",
            "292/298 [============================>.] - ETA: 0s - loss: 0.0042 - accuracy: 0.9958\n",
            "Epoch 00288: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0041 - accuracy: 0.9959 - val_loss: 0.0214 - val_accuracy: 0.9770\n",
            "Epoch 289/500\n",
            "294/298 [============================>.] - ETA: 0s - loss: 0.0041 - accuracy: 0.9959\n",
            "Epoch 00289: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0041 - accuracy: 0.9959 - val_loss: 0.0213 - val_accuracy: 0.9770\n",
            "Epoch 290/500\n",
            "295/298 [============================>.] - ETA: 0s - loss: 0.0040 - accuracy: 0.9960\n",
            "Epoch 00290: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0041 - accuracy: 0.9959 - val_loss: 0.0215 - val_accuracy: 0.9770\n",
            "Epoch 291/500\n",
            "291/298 [============================>.] - ETA: 0s - loss: 0.0041 - accuracy: 0.9959\n",
            "Epoch 00291: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0041 - accuracy: 0.9959 - val_loss: 0.0213 - val_accuracy: 0.9770\n",
            "Epoch 292/500\n",
            "289/298 [============================>.] - ETA: 0s - loss: 0.0041 - accuracy: 0.9959\n",
            "Epoch 00292: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0041 - accuracy: 0.9959 - val_loss: 0.0215 - val_accuracy: 0.9770\n",
            "Epoch 293/500\n",
            "294/298 [============================>.] - ETA: 0s - loss: 0.0041 - accuracy: 0.9959\n",
            "Epoch 00293: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0041 - accuracy: 0.9959 - val_loss: 0.0217 - val_accuracy: 0.9770\n",
            "Epoch 294/500\n",
            "278/298 [==========================>...] - ETA: 0s - loss: 0.0038 - accuracy: 0.9962\n",
            "Epoch 00294: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0041 - accuracy: 0.9959 - val_loss: 0.0213 - val_accuracy: 0.9778\n",
            "Epoch 295/500\n",
            "284/298 [===========================>..] - ETA: 0s - loss: 0.0038 - accuracy: 0.9961\n",
            "Epoch 00295: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0041 - accuracy: 0.9959 - val_loss: 0.0214 - val_accuracy: 0.9778\n",
            "Epoch 296/500\n",
            "294/298 [============================>.] - ETA: 0s - loss: 0.0040 - accuracy: 0.9960\n",
            "Epoch 00296: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0041 - accuracy: 0.9959 - val_loss: 0.0217 - val_accuracy: 0.9778\n",
            "Epoch 297/500\n",
            "281/298 [===========================>..] - ETA: 0s - loss: 0.0041 - accuracy: 0.9959\n",
            "Epoch 00297: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0041 - accuracy: 0.9959 - val_loss: 0.0216 - val_accuracy: 0.9778\n",
            "Epoch 298/500\n",
            "296/298 [============================>.] - ETA: 0s - loss: 0.0050 - accuracy: 0.9948\n",
            "Epoch 00298: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0049 - accuracy: 0.9949 - val_loss: 0.0191 - val_accuracy: 0.9802\n",
            "Epoch 299/500\n",
            "289/298 [============================>.] - ETA: 0s - loss: 0.0045 - accuracy: 0.9952\n",
            "Epoch 00299: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0046 - accuracy: 0.9952 - val_loss: 0.0280 - val_accuracy: 0.9690\n",
            "Epoch 300/500\n",
            "285/298 [===========================>..] - ETA: 0s - loss: 0.0040 - accuracy: 0.9961\n",
            "Epoch 00300: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0039 - accuracy: 0.9960 - val_loss: 0.0204 - val_accuracy: 0.9778\n",
            "Epoch 301/500\n",
            "296/298 [============================>.] - ETA: 0s - loss: 0.0038 - accuracy: 0.9961\n",
            "Epoch 00301: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0039 - accuracy: 0.9960 - val_loss: 0.0237 - val_accuracy: 0.9746\n",
            "Epoch 302/500\n",
            "297/298 [============================>.] - ETA: 0s - loss: 0.0039 - accuracy: 0.9961\n",
            "Epoch 00302: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0238 - val_accuracy: 0.9754\n",
            "Epoch 303/500\n",
            "289/298 [============================>.] - ETA: 0s - loss: 0.0040 - accuracy: 0.9960\n",
            "Epoch 00303: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0237 - val_accuracy: 0.9754\n",
            "Epoch 304/500\n",
            "294/298 [============================>.] - ETA: 0s - loss: 0.0038 - accuracy: 0.9962\n",
            "Epoch 00304: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0238 - val_accuracy: 0.9754\n",
            "Epoch 305/500\n",
            "286/298 [===========================>..] - ETA: 0s - loss: 0.0037 - accuracy: 0.9963\n",
            "Epoch 00305: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0237 - val_accuracy: 0.9754\n",
            "Epoch 306/500\n",
            "291/298 [============================>.] - ETA: 0s - loss: 0.0039 - accuracy: 0.9961\n",
            "Epoch 00306: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0236 - val_accuracy: 0.9754\n",
            "Epoch 307/500\n",
            "282/298 [===========================>..] - ETA: 0s - loss: 0.0038 - accuracy: 0.9962\n",
            "Epoch 00307: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0237 - val_accuracy: 0.9754\n",
            "Epoch 308/500\n",
            "295/298 [============================>.] - ETA: 0s - loss: 0.0038 - accuracy: 0.9962\n",
            "Epoch 00308: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0236 - val_accuracy: 0.9754\n",
            "Epoch 309/500\n",
            "294/298 [============================>.] - ETA: 0s - loss: 0.0039 - accuracy: 0.9961\n",
            "Epoch 00309: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0237 - val_accuracy: 0.9754\n",
            "Epoch 310/500\n",
            "295/298 [============================>.] - ETA: 0s - loss: 0.0039 - accuracy: 0.9961\n",
            "Epoch 00310: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0236 - val_accuracy: 0.9754\n",
            "Epoch 311/500\n",
            "288/298 [===========================>..] - ETA: 0s - loss: 0.0040 - accuracy: 0.9960\n",
            "Epoch 00311: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0235 - val_accuracy: 0.9754\n",
            "Epoch 312/500\n",
            "293/298 [============================>.] - ETA: 0s - loss: 0.0037 - accuracy: 0.9963\n",
            "Epoch 00312: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0233 - val_accuracy: 0.9754\n",
            "Epoch 313/500\n",
            "297/298 [============================>.] - ETA: 0s - loss: 0.0039 - accuracy: 0.9961\n",
            "Epoch 00313: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0232 - val_accuracy: 0.9754\n",
            "Epoch 314/500\n",
            "295/298 [============================>.] - ETA: 0s - loss: 0.0038 - accuracy: 0.9962\n",
            "Epoch 00314: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0234 - val_accuracy: 0.9754\n",
            "Epoch 315/500\n",
            "292/298 [============================>.] - ETA: 0s - loss: 0.0039 - accuracy: 0.9960\n",
            "Epoch 00315: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0230 - val_accuracy: 0.9754\n",
            "Epoch 316/500\n",
            "289/298 [============================>.] - ETA: 0s - loss: 0.0039 - accuracy: 0.9961\n",
            "Epoch 00316: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0233 - val_accuracy: 0.9754\n",
            "Epoch 317/500\n",
            "296/298 [============================>.] - ETA: 0s - loss: 0.0039 - accuracy: 0.9961\n",
            "Epoch 00317: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0230 - val_accuracy: 0.9754\n",
            "Epoch 318/500\n",
            "292/298 [============================>.] - ETA: 0s - loss: 0.0037 - accuracy: 0.9963\n",
            "Epoch 00318: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0233 - val_accuracy: 0.9754\n",
            "Epoch 319/500\n",
            "288/298 [===========================>..] - ETA: 0s - loss: 0.0039 - accuracy: 0.9961\n",
            "Epoch 00319: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0229 - val_accuracy: 0.9754\n",
            "Epoch 320/500\n",
            "286/298 [===========================>..] - ETA: 0s - loss: 0.0040 - accuracy: 0.9960\n",
            "Epoch 00320: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0229 - val_accuracy: 0.9754\n",
            "Epoch 321/500\n",
            "298/298 [==============================] - ETA: 0s - loss: 0.0039 - accuracy: 0.9961\n",
            "Epoch 00321: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0228 - val_accuracy: 0.9754\n",
            "Epoch 322/500\n",
            "298/298 [==============================] - ETA: 0s - loss: 0.0039 - accuracy: 0.9961\n",
            "Epoch 00322: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0233 - val_accuracy: 0.9754\n",
            "Epoch 323/500\n",
            "294/298 [============================>.] - ETA: 0s - loss: 0.0039 - accuracy: 0.9961\n",
            "Epoch 00323: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0227 - val_accuracy: 0.9754\n",
            "Epoch 324/500\n",
            "291/298 [============================>.] - ETA: 0s - loss: 0.0040 - accuracy: 0.9960\n",
            "Epoch 00324: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0220 - val_accuracy: 0.9770\n",
            "Epoch 325/500\n",
            "282/298 [===========================>..] - ETA: 0s - loss: 0.0040 - accuracy: 0.9960\n",
            "Epoch 00325: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0224 - val_accuracy: 0.9762\n",
            "Epoch 326/500\n",
            "292/298 [============================>.] - ETA: 0s - loss: 0.0038 - accuracy: 0.9961\n",
            "Epoch 00326: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0220 - val_accuracy: 0.9770\n",
            "Epoch 327/500\n",
            "293/298 [============================>.] - ETA: 0s - loss: 0.0039 - accuracy: 0.9961\n",
            "Epoch 00327: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0219 - val_accuracy: 0.9770\n",
            "Epoch 328/500\n",
            "289/298 [============================>.] - ETA: 0s - loss: 0.0040 - accuracy: 0.9960\n",
            "Epoch 00328: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0218 - val_accuracy: 0.9770\n",
            "Epoch 329/500\n",
            "288/298 [===========================>..] - ETA: 0s - loss: 0.0039 - accuracy: 0.9961\n",
            "Epoch 00329: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0229 - val_accuracy: 0.9754\n",
            "Epoch 330/500\n",
            "296/298 [============================>.] - ETA: 0s - loss: 0.0039 - accuracy: 0.9961\n",
            "Epoch 00330: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0215 - val_accuracy: 0.9770\n",
            "Epoch 331/500\n",
            "279/298 [===========================>..] - ETA: 0s - loss: 0.0038 - accuracy: 0.9962\n",
            "Epoch 00331: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0217 - val_accuracy: 0.9770\n",
            "Epoch 332/500\n",
            "294/298 [============================>.] - ETA: 0s - loss: 0.0039 - accuracy: 0.9961\n",
            "Epoch 00332: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0217 - val_accuracy: 0.9770\n",
            "Epoch 333/500\n",
            "288/298 [===========================>..] - ETA: 0s - loss: 0.0038 - accuracy: 0.9962\n",
            "Epoch 00333: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0216 - val_accuracy: 0.9770\n",
            "Epoch 334/500\n",
            "296/298 [============================>.] - ETA: 0s - loss: 0.0039 - accuracy: 0.9961\n",
            "Epoch 00334: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0213 - val_accuracy: 0.9786\n",
            "Epoch 335/500\n",
            "279/298 [===========================>..] - ETA: 0s - loss: 0.0038 - accuracy: 0.9962\n",
            "Epoch 00335: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0219 - val_accuracy: 0.9762\n",
            "Epoch 336/500\n",
            "298/298 [==============================] - ETA: 0s - loss: 0.0049 - accuracy: 0.9950\n",
            "Epoch 00336: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0049 - accuracy: 0.9950 - val_loss: 0.0172 - val_accuracy: 0.9817\n",
            "Epoch 337/500\n",
            "279/298 [===========================>..] - ETA: 0s - loss: 0.0041 - accuracy: 0.9959\n",
            "Epoch 00337: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0041 - accuracy: 0.9959 - val_loss: 0.0217 - val_accuracy: 0.9778\n",
            "Epoch 338/500\n",
            "298/298 [==============================] - ETA: 0s - loss: 0.0040 - accuracy: 0.9958\n",
            "Epoch 00338: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0040 - accuracy: 0.9958 - val_loss: 0.0232 - val_accuracy: 0.9754\n",
            "Epoch 339/500\n",
            "278/298 [==========================>...] - ETA: 0s - loss: 0.0039 - accuracy: 0.9961\n",
            "Epoch 00339: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0222 - val_accuracy: 0.9754\n",
            "Epoch 340/500\n",
            "286/298 [===========================>..] - ETA: 0s - loss: 0.0037 - accuracy: 0.9963\n",
            "Epoch 00340: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0221 - val_accuracy: 0.9754\n",
            "Epoch 341/500\n",
            "289/298 [============================>.] - ETA: 0s - loss: 0.0037 - accuracy: 0.9963\n",
            "Epoch 00341: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0219 - val_accuracy: 0.9754\n",
            "Epoch 342/500\n",
            "282/298 [===========================>..] - ETA: 0s - loss: 0.0041 - accuracy: 0.9959\n",
            "Epoch 00342: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0219 - val_accuracy: 0.9754\n",
            "Epoch 343/500\n",
            "289/298 [============================>.] - ETA: 0s - loss: 0.0040 - accuracy: 0.9960\n",
            "Epoch 00343: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0217 - val_accuracy: 0.9770\n",
            "Epoch 344/500\n",
            "287/298 [===========================>..] - ETA: 0s - loss: 0.0038 - accuracy: 0.9962\n",
            "Epoch 00344: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0217 - val_accuracy: 0.9762\n",
            "Epoch 345/500\n",
            "289/298 [============================>.] - ETA: 0s - loss: 0.0040 - accuracy: 0.9960\n",
            "Epoch 00345: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0216 - val_accuracy: 0.9762\n",
            "Epoch 346/500\n",
            "280/298 [===========================>..] - ETA: 0s - loss: 0.0036 - accuracy: 0.9964\n",
            "Epoch 00346: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0215 - val_accuracy: 0.9770\n",
            "Epoch 347/500\n",
            "292/298 [============================>.] - ETA: 0s - loss: 0.0038 - accuracy: 0.9961\n",
            "Epoch 00347: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0215 - val_accuracy: 0.9770\n",
            "Epoch 348/500\n",
            "291/298 [============================>.] - ETA: 0s - loss: 0.0040 - accuracy: 0.9960\n",
            "Epoch 00348: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0214 - val_accuracy: 0.9778\n",
            "Epoch 349/500\n",
            "291/298 [============================>.] - ETA: 0s - loss: 0.0040 - accuracy: 0.9960\n",
            "Epoch 00349: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0213 - val_accuracy: 0.9770\n",
            "Epoch 350/500\n",
            "291/298 [============================>.] - ETA: 0s - loss: 0.0040 - accuracy: 0.9960\n",
            "Epoch 00350: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0214 - val_accuracy: 0.9770\n",
            "Epoch 351/500\n",
            "289/298 [============================>.] - ETA: 0s - loss: 0.0039 - accuracy: 0.9961\n",
            "Epoch 00351: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0211 - val_accuracy: 0.9778\n",
            "Epoch 352/500\n",
            "288/298 [===========================>..] - ETA: 0s - loss: 0.0038 - accuracy: 0.9962\n",
            "Epoch 00352: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0213 - val_accuracy: 0.9770\n",
            "Epoch 353/500\n",
            "294/298 [============================>.] - ETA: 0s - loss: 0.0039 - accuracy: 0.9961\n",
            "Epoch 00353: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0211 - val_accuracy: 0.9778\n",
            "Epoch 354/500\n",
            "292/298 [============================>.] - ETA: 0s - loss: 0.0037 - accuracy: 0.9963\n",
            "Epoch 00354: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0210 - val_accuracy: 0.9778\n",
            "Epoch 355/500\n",
            "292/298 [============================>.] - ETA: 0s - loss: 0.0038 - accuracy: 0.9961\n",
            "Epoch 00355: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0212 - val_accuracy: 0.9770\n",
            "Epoch 356/500\n",
            "296/298 [============================>.] - ETA: 0s - loss: 0.0039 - accuracy: 0.9961\n",
            "Epoch 00356: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0212 - val_accuracy: 0.9778\n",
            "Epoch 357/500\n",
            "292/298 [============================>.] - ETA: 0s - loss: 0.0037 - accuracy: 0.9963\n",
            "Epoch 00357: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0208 - val_accuracy: 0.9778\n",
            "Epoch 358/500\n",
            "296/298 [============================>.] - ETA: 0s - loss: 0.0039 - accuracy: 0.9961\n",
            "Epoch 00358: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0207 - val_accuracy: 0.9778\n",
            "Epoch 359/500\n",
            "290/298 [============================>.] - ETA: 0s - loss: 0.0040 - accuracy: 0.9960\n",
            "Epoch 00359: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0205 - val_accuracy: 0.9786\n",
            "Epoch 360/500\n",
            "297/298 [============================>.] - ETA: 0s - loss: 0.0039 - accuracy: 0.9961\n",
            "Epoch 00360: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0208 - val_accuracy: 0.9778\n",
            "Epoch 361/500\n",
            "287/298 [===========================>..] - ETA: 0s - loss: 0.0040 - accuracy: 0.9960\n",
            "Epoch 00361: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0206 - val_accuracy: 0.9778\n",
            "Epoch 362/500\n",
            "292/298 [============================>.] - ETA: 0s - loss: 0.0038 - accuracy: 0.9961\n",
            "Epoch 00362: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0203 - val_accuracy: 0.9778\n",
            "Epoch 363/500\n",
            "291/298 [============================>.] - ETA: 0s - loss: 0.0038 - accuracy: 0.9961\n",
            "Epoch 00363: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0203 - val_accuracy: 0.9778\n",
            "Epoch 364/500\n",
            "291/298 [============================>.] - ETA: 0s - loss: 0.0048 - accuracy: 0.9952\n",
            "Epoch 00364: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0047 - accuracy: 0.9953 - val_loss: 0.0326 - val_accuracy: 0.9659\n",
            "Epoch 365/500\n",
            "290/298 [============================>.] - ETA: 0s - loss: 0.0044 - accuracy: 0.9955\n",
            "Epoch 00365: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0043 - accuracy: 0.9956 - val_loss: 0.0243 - val_accuracy: 0.9738\n",
            "Epoch 366/500\n",
            "287/298 [===========================>..] - ETA: 0s - loss: 0.0037 - accuracy: 0.9963\n",
            "Epoch 00366: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0243 - val_accuracy: 0.9738\n",
            "Epoch 367/500\n",
            "289/298 [============================>.] - ETA: 0s - loss: 0.0040 - accuracy: 0.9960\n",
            "Epoch 00367: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0244 - val_accuracy: 0.9738\n",
            "Epoch 368/500\n",
            "287/298 [===========================>..] - ETA: 0s - loss: 0.0039 - accuracy: 0.9961\n",
            "Epoch 00368: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0244 - val_accuracy: 0.9738\n",
            "Epoch 369/500\n",
            "287/298 [===========================>..] - ETA: 0s - loss: 0.0039 - accuracy: 0.9961\n",
            "Epoch 00369: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0245 - val_accuracy: 0.9738\n",
            "Epoch 370/500\n",
            "289/298 [============================>.] - ETA: 0s - loss: 0.0037 - accuracy: 0.9963\n",
            "Epoch 00370: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0244 - val_accuracy: 0.9738\n",
            "Epoch 371/500\n",
            "292/298 [============================>.] - ETA: 0s - loss: 0.0039 - accuracy: 0.9960\n",
            "Epoch 00371: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0244 - val_accuracy: 0.9738\n",
            "Epoch 372/500\n",
            "283/298 [===========================>..] - ETA: 0s - loss: 0.0038 - accuracy: 0.9961\n",
            "Epoch 00372: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0244 - val_accuracy: 0.9738\n",
            "Epoch 373/500\n",
            "297/298 [============================>.] - ETA: 0s - loss: 0.0039 - accuracy: 0.9961\n",
            "Epoch 00373: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0245 - val_accuracy: 0.9738\n",
            "Epoch 374/500\n",
            "283/298 [===========================>..] - ETA: 0s - loss: 0.0041 - accuracy: 0.9959\n",
            "Epoch 00374: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0245 - val_accuracy: 0.9738\n",
            "Epoch 375/500\n",
            "282/298 [===========================>..] - ETA: 0s - loss: 0.0040 - accuracy: 0.9960\n",
            "Epoch 00375: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0244 - val_accuracy: 0.9738\n",
            "Epoch 376/500\n",
            "292/298 [============================>.] - ETA: 0s - loss: 0.0039 - accuracy: 0.9960\n",
            "Epoch 00376: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0243 - val_accuracy: 0.9738\n",
            "Epoch 377/500\n",
            "285/298 [===========================>..] - ETA: 0s - loss: 0.0038 - accuracy: 0.9962\n",
            "Epoch 00377: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0243 - val_accuracy: 0.9738\n",
            "Epoch 378/500\n",
            "282/298 [===========================>..] - ETA: 0s - loss: 0.0039 - accuracy: 0.9961\n",
            "Epoch 00378: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0242 - val_accuracy: 0.9738\n",
            "Epoch 379/500\n",
            "289/298 [============================>.] - ETA: 0s - loss: 0.0039 - accuracy: 0.9961\n",
            "Epoch 00379: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0242 - val_accuracy: 0.9730\n",
            "Epoch 380/500\n",
            "291/298 [============================>.] - ETA: 0s - loss: 0.0040 - accuracy: 0.9960\n",
            "Epoch 00380: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0240 - val_accuracy: 0.9730\n",
            "Epoch 381/500\n",
            "284/298 [===========================>..] - ETA: 0s - loss: 0.0039 - accuracy: 0.9960\n",
            "Epoch 00381: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0242 - val_accuracy: 0.9730\n",
            "Epoch 382/500\n",
            "280/298 [===========================>..] - ETA: 0s - loss: 0.0040 - accuracy: 0.9960\n",
            "Epoch 00382: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0241 - val_accuracy: 0.9730\n",
            "Epoch 383/500\n",
            "284/298 [===========================>..] - ETA: 0s - loss: 0.0039 - accuracy: 0.9960\n",
            "Epoch 00383: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0239 - val_accuracy: 0.9730\n",
            "Epoch 384/500\n",
            "290/298 [============================>.] - ETA: 0s - loss: 0.0039 - accuracy: 0.9961\n",
            "Epoch 00384: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0244 - val_accuracy: 0.9730\n",
            "Epoch 385/500\n",
            "294/298 [============================>.] - ETA: 0s - loss: 0.0039 - accuracy: 0.9961\n",
            "Epoch 00385: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0242 - val_accuracy: 0.9730\n",
            "Epoch 386/500\n",
            "287/298 [===========================>..] - ETA: 0s - loss: 0.0038 - accuracy: 0.9962\n",
            "Epoch 00386: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0240 - val_accuracy: 0.9730\n",
            "Epoch 387/500\n",
            "280/298 [===========================>..] - ETA: 0s - loss: 0.0040 - accuracy: 0.9960\n",
            "Epoch 00387: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0244 - val_accuracy: 0.9722\n",
            "Epoch 388/500\n",
            "297/298 [============================>.] - ETA: 0s - loss: 0.0039 - accuracy: 0.9961\n",
            "Epoch 00388: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0239 - val_accuracy: 0.9746\n",
            "Epoch 389/500\n",
            "296/298 [============================>.] - ETA: 0s - loss: 0.0039 - accuracy: 0.9961\n",
            "Epoch 00389: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0237 - val_accuracy: 0.9746\n",
            "Epoch 390/500\n",
            "279/298 [===========================>..] - ETA: 0s - loss: 0.0038 - accuracy: 0.9962\n",
            "Epoch 00390: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0235 - val_accuracy: 0.9738\n",
            "Epoch 391/500\n",
            "287/298 [===========================>..] - ETA: 0s - loss: 0.0040 - accuracy: 0.9960\n",
            "Epoch 00391: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0241 - val_accuracy: 0.9730\n",
            "Epoch 392/500\n",
            "283/298 [===========================>..] - ETA: 0s - loss: 0.0035 - accuracy: 0.9965\n",
            "Epoch 00392: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0240 - val_accuracy: 0.9730\n",
            "Epoch 393/500\n",
            "284/298 [===========================>..] - ETA: 0s - loss: 0.0035 - accuracy: 0.9965\n",
            "Epoch 00393: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0233 - val_accuracy: 0.9738\n",
            "Epoch 394/500\n",
            "296/298 [============================>.] - ETA: 0s - loss: 0.0039 - accuracy: 0.9961\n",
            "Epoch 00394: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0232 - val_accuracy: 0.9738\n",
            "Epoch 395/500\n",
            "295/298 [============================>.] - ETA: 0s - loss: 0.0039 - accuracy: 0.9961\n",
            "Epoch 00395: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0232 - val_accuracy: 0.9730\n",
            "Epoch 396/500\n",
            "291/298 [============================>.] - ETA: 0s - loss: 0.0037 - accuracy: 0.9962\n",
            "Epoch 00396: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0237 - val_accuracy: 0.9730\n",
            "Epoch 397/500\n",
            "287/298 [===========================>..] - ETA: 0s - loss: 0.0038 - accuracy: 0.9962\n",
            "Epoch 00397: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0231 - val_accuracy: 0.9746\n",
            "Epoch 398/500\n",
            "294/298 [============================>.] - ETA: 0s - loss: 0.0041 - accuracy: 0.9960\n",
            "Epoch 00398: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0041 - accuracy: 0.9959 - val_loss: 0.0363 - val_accuracy: 0.9619\n",
            "Epoch 399/500\n",
            "294/298 [============================>.] - ETA: 0s - loss: 0.0048 - accuracy: 0.9949\n",
            "Epoch 00399: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0048 - accuracy: 0.9950 - val_loss: 0.0176 - val_accuracy: 0.9817\n",
            "Epoch 400/500\n",
            "285/298 [===========================>..] - ETA: 0s - loss: 0.0036 - accuracy: 0.9964\n",
            "Epoch 00400: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0040 - accuracy: 0.9960 - val_loss: 0.0236 - val_accuracy: 0.9746\n",
            "Epoch 401/500\n",
            "289/298 [============================>.] - ETA: 0s - loss: 0.0039 - accuracy: 0.9961\n",
            "Epoch 00401: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0250 - val_accuracy: 0.9738\n",
            "Epoch 402/500\n",
            "295/298 [============================>.] - ETA: 0s - loss: 0.0039 - accuracy: 0.9961\n",
            "Epoch 00402: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0244 - val_accuracy: 0.9738\n",
            "Epoch 403/500\n",
            "291/298 [============================>.] - ETA: 0s - loss: 0.0038 - accuracy: 0.9961\n",
            "Epoch 00403: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0240 - val_accuracy: 0.9746\n",
            "Epoch 404/500\n",
            "286/298 [===========================>..] - ETA: 0s - loss: 0.0038 - accuracy: 0.9962\n",
            "Epoch 00404: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0237 - val_accuracy: 0.9746\n",
            "Epoch 405/500\n",
            "283/298 [===========================>..] - ETA: 0s - loss: 0.0040 - accuracy: 0.9960\n",
            "Epoch 00405: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0234 - val_accuracy: 0.9746\n",
            "Epoch 406/500\n",
            "276/298 [==========================>...] - ETA: 0s - loss: 0.0037 - accuracy: 0.9963\n",
            "Epoch 00406: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0232 - val_accuracy: 0.9746\n",
            "Epoch 407/500\n",
            "287/298 [===========================>..] - ETA: 0s - loss: 0.0038 - accuracy: 0.9962\n",
            "Epoch 00407: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0229 - val_accuracy: 0.9754\n",
            "Epoch 408/500\n",
            "282/298 [===========================>..] - ETA: 0s - loss: 0.0040 - accuracy: 0.9960\n",
            "Epoch 00408: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0227 - val_accuracy: 0.9762\n",
            "Epoch 409/500\n",
            "295/298 [============================>.] - ETA: 0s - loss: 0.0037 - accuracy: 0.9963\n",
            "Epoch 00409: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0225 - val_accuracy: 0.9762\n",
            "Epoch 410/500\n",
            "287/298 [===========================>..] - ETA: 0s - loss: 0.0038 - accuracy: 0.9962\n",
            "Epoch 00410: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0223 - val_accuracy: 0.9770\n",
            "Epoch 411/500\n",
            "295/298 [============================>.] - ETA: 0s - loss: 0.0039 - accuracy: 0.9961\n",
            "Epoch 00411: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0222 - val_accuracy: 0.9762\n",
            "Epoch 412/500\n",
            "293/298 [============================>.] - ETA: 0s - loss: 0.0039 - accuracy: 0.9961\n",
            "Epoch 00412: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0220 - val_accuracy: 0.9762\n",
            "Epoch 413/500\n",
            "285/298 [===========================>..] - ETA: 0s - loss: 0.0039 - accuracy: 0.9961\n",
            "Epoch 00413: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0219 - val_accuracy: 0.9762\n",
            "Epoch 414/500\n",
            "294/298 [============================>.] - ETA: 0s - loss: 0.0039 - accuracy: 0.9961\n",
            "Epoch 00414: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0218 - val_accuracy: 0.9762\n",
            "Epoch 415/500\n",
            "286/298 [===========================>..] - ETA: 0s - loss: 0.0038 - accuracy: 0.9962\n",
            "Epoch 00415: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0218 - val_accuracy: 0.9762\n",
            "Epoch 416/500\n",
            "287/298 [===========================>..] - ETA: 0s - loss: 0.0040 - accuracy: 0.9960\n",
            "Epoch 00416: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0217 - val_accuracy: 0.9754\n",
            "Epoch 417/500\n",
            "292/298 [============================>.] - ETA: 0s - loss: 0.0039 - accuracy: 0.9960\n",
            "Epoch 00417: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0217 - val_accuracy: 0.9754\n",
            "Epoch 418/500\n",
            "290/298 [============================>.] - ETA: 0s - loss: 0.0040 - accuracy: 0.9960\n",
            "Epoch 00418: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0216 - val_accuracy: 0.9754\n",
            "Epoch 419/500\n",
            "294/298 [============================>.] - ETA: 0s - loss: 0.0039 - accuracy: 0.9961\n",
            "Epoch 00419: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0216 - val_accuracy: 0.9754\n",
            "Epoch 420/500\n",
            "289/298 [============================>.] - ETA: 0s - loss: 0.0038 - accuracy: 0.9962\n",
            "Epoch 00420: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0218 - val_accuracy: 0.9754\n",
            "Epoch 421/500\n",
            "289/298 [============================>.] - ETA: 0s - loss: 0.0040 - accuracy: 0.9960\n",
            "Epoch 00421: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0217 - val_accuracy: 0.9754\n",
            "Epoch 422/500\n",
            "288/298 [===========================>..] - ETA: 0s - loss: 0.0038 - accuracy: 0.9962\n",
            "Epoch 00422: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0215 - val_accuracy: 0.9754\n",
            "Epoch 423/500\n",
            "285/298 [===========================>..] - ETA: 0s - loss: 0.0039 - accuracy: 0.9961\n",
            "Epoch 00423: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0219 - val_accuracy: 0.9754\n",
            "Epoch 424/500\n",
            "288/298 [===========================>..] - ETA: 0s - loss: 0.0039 - accuracy: 0.9961\n",
            "Epoch 00424: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0221 - val_accuracy: 0.9754\n",
            "Epoch 425/500\n",
            "286/298 [===========================>..] - ETA: 0s - loss: 0.0040 - accuracy: 0.9960\n",
            "Epoch 00425: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9961 - val_loss: 0.0222 - val_accuracy: 0.9762\n",
            "Epoch 426/500\n",
            "286/298 [===========================>..] - ETA: 0s - loss: 0.0051 - accuracy: 0.9946\n",
            "Epoch 00426: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0052 - accuracy: 0.9944 - val_loss: 0.0240 - val_accuracy: 0.9730\n",
            "Epoch 427/500\n",
            "291/298 [============================>.] - ETA: 0s - loss: 0.0041 - accuracy: 0.9956\n",
            "Epoch 00427: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0042 - accuracy: 0.9956 - val_loss: 0.0215 - val_accuracy: 0.9778\n",
            "Epoch 428/500\n",
            "297/298 [============================>.] - ETA: 0s - loss: 0.0038 - accuracy: 0.9962\n",
            "Epoch 00428: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0038 - accuracy: 0.9962 - val_loss: 0.0223 - val_accuracy: 0.9762\n",
            "Epoch 429/500\n",
            "295/298 [============================>.] - ETA: 0s - loss: 0.0038 - accuracy: 0.9962\n",
            "Epoch 00429: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 2ms/step - loss: 0.0038 - accuracy: 0.9962 - val_loss: 0.0223 - val_accuracy: 0.9762\n",
            "Epoch 430/500\n",
            "294/298 [============================>.] - ETA: 0s - loss: 0.0036 - accuracy: 0.9964\n",
            "Epoch 00430: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0038 - accuracy: 0.9962 - val_loss: 0.0223 - val_accuracy: 0.9762\n",
            "Epoch 431/500\n",
            "297/298 [============================>.] - ETA: 0s - loss: 0.0038 - accuracy: 0.9962\n",
            "Epoch 00431: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0038 - accuracy: 0.9962 - val_loss: 0.0223 - val_accuracy: 0.9762\n",
            "Epoch 432/500\n",
            "287/298 [===========================>..] - ETA: 0s - loss: 0.0038 - accuracy: 0.9962\n",
            "Epoch 00432: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0038 - accuracy: 0.9962 - val_loss: 0.0223 - val_accuracy: 0.9762\n",
            "Epoch 433/500\n",
            "285/298 [===========================>..] - ETA: 0s - loss: 0.0037 - accuracy: 0.9963\n",
            "Epoch 00433: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0038 - accuracy: 0.9962 - val_loss: 0.0224 - val_accuracy: 0.9762\n",
            "Epoch 434/500\n",
            "297/298 [============================>.] - ETA: 0s - loss: 0.0038 - accuracy: 0.9962\n",
            "Epoch 00434: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0038 - accuracy: 0.9962 - val_loss: 0.0224 - val_accuracy: 0.9762\n",
            "Epoch 435/500\n",
            "296/298 [============================>.] - ETA: 0s - loss: 0.0038 - accuracy: 0.9962\n",
            "Epoch 00435: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0038 - accuracy: 0.9962 - val_loss: 0.0225 - val_accuracy: 0.9762\n",
            "Epoch 436/500\n",
            "283/298 [===========================>..] - ETA: 0s - loss: 0.0037 - accuracy: 0.9962\n",
            "Epoch 00436: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0038 - accuracy: 0.9962 - val_loss: 0.0225 - val_accuracy: 0.9762\n",
            "Epoch 437/500\n",
            "277/298 [==========================>...] - ETA: 0s - loss: 0.0038 - accuracy: 0.9962\n",
            "Epoch 00437: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0038 - accuracy: 0.9962 - val_loss: 0.0225 - val_accuracy: 0.9762\n",
            "Epoch 438/500\n",
            "290/298 [============================>.] - ETA: 0s - loss: 0.0039 - accuracy: 0.9961\n",
            "Epoch 00438: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0038 - accuracy: 0.9962 - val_loss: 0.0225 - val_accuracy: 0.9762\n",
            "Epoch 439/500\n",
            "297/298 [============================>.] - ETA: 0s - loss: 0.0038 - accuracy: 0.9962\n",
            "Epoch 00439: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0038 - accuracy: 0.9962 - val_loss: 0.0226 - val_accuracy: 0.9762\n",
            "Epoch 440/500\n",
            "285/298 [===========================>..] - ETA: 0s - loss: 0.0037 - accuracy: 0.9963\n",
            "Epoch 00440: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0038 - accuracy: 0.9962 - val_loss: 0.0226 - val_accuracy: 0.9762\n",
            "Epoch 441/500\n",
            "275/298 [==========================>...] - ETA: 0s - loss: 0.0035 - accuracy: 0.9965\n",
            "Epoch 00441: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0038 - accuracy: 0.9962 - val_loss: 0.0226 - val_accuracy: 0.9762\n",
            "Epoch 442/500\n",
            "287/298 [===========================>..] - ETA: 0s - loss: 0.0037 - accuracy: 0.9963\n",
            "Epoch 00442: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0038 - accuracy: 0.9962 - val_loss: 0.0227 - val_accuracy: 0.9762\n",
            "Epoch 443/500\n",
            "294/298 [============================>.] - ETA: 0s - loss: 0.0037 - accuracy: 0.9963\n",
            "Epoch 00443: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0038 - accuracy: 0.9962 - val_loss: 0.0226 - val_accuracy: 0.9762\n",
            "Epoch 444/500\n",
            "296/298 [============================>.] - ETA: 0s - loss: 0.0038 - accuracy: 0.9962\n",
            "Epoch 00444: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0038 - accuracy: 0.9962 - val_loss: 0.0227 - val_accuracy: 0.9762\n",
            "Epoch 445/500\n",
            "283/298 [===========================>..] - ETA: 0s - loss: 0.0037 - accuracy: 0.9962\n",
            "Epoch 00445: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0038 - accuracy: 0.9962 - val_loss: 0.0227 - val_accuracy: 0.9762\n",
            "Epoch 446/500\n",
            "298/298 [==============================] - ETA: 0s - loss: 0.0038 - accuracy: 0.9962\n",
            "Epoch 00446: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0038 - accuracy: 0.9962 - val_loss: 0.0226 - val_accuracy: 0.9762\n",
            "Epoch 447/500\n",
            "285/298 [===========================>..] - ETA: 0s - loss: 0.0038 - accuracy: 0.9962\n",
            "Epoch 00447: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0038 - accuracy: 0.9962 - val_loss: 0.0229 - val_accuracy: 0.9754\n",
            "Epoch 448/500\n",
            "295/298 [============================>.] - ETA: 0s - loss: 0.0038 - accuracy: 0.9962\n",
            "Epoch 00448: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0038 - accuracy: 0.9962 - val_loss: 0.0229 - val_accuracy: 0.9754\n",
            "Epoch 449/500\n",
            "290/298 [============================>.] - ETA: 0s - loss: 0.0039 - accuracy: 0.9961\n",
            "Epoch 00449: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0038 - accuracy: 0.9962 - val_loss: 0.0228 - val_accuracy: 0.9754\n",
            "Epoch 450/500\n",
            "295/298 [============================>.] - ETA: 0s - loss: 0.0038 - accuracy: 0.9962\n",
            "Epoch 00450: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0038 - accuracy: 0.9962 - val_loss: 0.0228 - val_accuracy: 0.9754\n",
            "Epoch 451/500\n",
            "289/298 [============================>.] - ETA: 0s - loss: 0.0039 - accuracy: 0.9961\n",
            "Epoch 00451: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0038 - accuracy: 0.9962 - val_loss: 0.0227 - val_accuracy: 0.9754\n",
            "Epoch 452/500\n",
            "281/298 [===========================>..] - ETA: 0s - loss: 0.0039 - accuracy: 0.9961\n",
            "Epoch 00452: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0038 - accuracy: 0.9962 - val_loss: 0.0230 - val_accuracy: 0.9754\n",
            "Epoch 453/500\n",
            "290/298 [============================>.] - ETA: 0s - loss: 0.0036 - accuracy: 0.9963\n",
            "Epoch 00453: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0038 - accuracy: 0.9962 - val_loss: 0.0229 - val_accuracy: 0.9754\n",
            "Epoch 454/500\n",
            "278/298 [==========================>...] - ETA: 0s - loss: 0.0038 - accuracy: 0.9962\n",
            "Epoch 00454: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0038 - accuracy: 0.9962 - val_loss: 0.0230 - val_accuracy: 0.9754\n",
            "Epoch 455/500\n",
            "298/298 [==============================] - ETA: 0s - loss: 0.0038 - accuracy: 0.9962\n",
            "Epoch 00455: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0038 - accuracy: 0.9962 - val_loss: 0.0235 - val_accuracy: 0.9746\n",
            "Epoch 456/500\n",
            "287/298 [===========================>..] - ETA: 0s - loss: 0.0039 - accuracy: 0.9961\n",
            "Epoch 00456: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0038 - accuracy: 0.9962 - val_loss: 0.0235 - val_accuracy: 0.9746\n",
            "Epoch 457/500\n",
            "287/298 [===========================>..] - ETA: 0s - loss: 0.0039 - accuracy: 0.9961\n",
            "Epoch 00457: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0038 - accuracy: 0.9962 - val_loss: 0.0235 - val_accuracy: 0.9746\n",
            "Epoch 458/500\n",
            "295/298 [============================>.] - ETA: 0s - loss: 0.0037 - accuracy: 0.9963\n",
            "Epoch 00458: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0038 - accuracy: 0.9962 - val_loss: 0.0224 - val_accuracy: 0.9762\n",
            "Epoch 459/500\n",
            "280/298 [===========================>..] - ETA: 0s - loss: 0.0041 - accuracy: 0.9958\n",
            "Epoch 00459: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0043 - accuracy: 0.9956 - val_loss: 0.0265 - val_accuracy: 0.9698\n",
            "Epoch 460/500\n",
            "294/298 [============================>.] - ETA: 0s - loss: 0.0038 - accuracy: 0.9962\n",
            "Epoch 00460: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0038 - accuracy: 0.9962 - val_loss: 0.0248 - val_accuracy: 0.9714\n",
            "Epoch 461/500\n",
            "286/298 [===========================>..] - ETA: 0s - loss: 0.0037 - accuracy: 0.9963\n",
            "Epoch 00461: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0038 - accuracy: 0.9962 - val_loss: 0.0249 - val_accuracy: 0.9714\n",
            "Epoch 462/500\n",
            "292/298 [============================>.] - ETA: 0s - loss: 0.0038 - accuracy: 0.9961\n",
            "Epoch 00462: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0038 - accuracy: 0.9962 - val_loss: 0.0250 - val_accuracy: 0.9714\n",
            "Epoch 463/500\n",
            "276/298 [==========================>...] - ETA: 0s - loss: 0.0036 - accuracy: 0.9964\n",
            "Epoch 00463: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0038 - accuracy: 0.9962 - val_loss: 0.0250 - val_accuracy: 0.9714\n",
            "Epoch 464/500\n",
            "293/298 [============================>.] - ETA: 0s - loss: 0.0037 - accuracy: 0.9963\n",
            "Epoch 00464: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0038 - accuracy: 0.9962 - val_loss: 0.0251 - val_accuracy: 0.9714\n",
            "Epoch 465/500\n",
            "296/298 [============================>.] - ETA: 0s - loss: 0.0038 - accuracy: 0.9962\n",
            "Epoch 00465: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0038 - accuracy: 0.9962 - val_loss: 0.0251 - val_accuracy: 0.9714\n",
            "Epoch 466/500\n",
            "292/298 [============================>.] - ETA: 0s - loss: 0.0038 - accuracy: 0.9961\n",
            "Epoch 00466: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0038 - accuracy: 0.9962 - val_loss: 0.0253 - val_accuracy: 0.9714\n",
            "Epoch 467/500\n",
            "297/298 [============================>.] - ETA: 0s - loss: 0.0038 - accuracy: 0.9962\n",
            "Epoch 00467: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0038 - accuracy: 0.9962 - val_loss: 0.0256 - val_accuracy: 0.9706\n",
            "Epoch 468/500\n",
            "289/298 [============================>.] - ETA: 0s - loss: 0.0038 - accuracy: 0.9962\n",
            "Epoch 00468: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0038 - accuracy: 0.9962 - val_loss: 0.0263 - val_accuracy: 0.9706\n",
            "Epoch 469/500\n",
            "278/298 [==========================>...] - ETA: 0s - loss: 0.0038 - accuracy: 0.9962\n",
            "Epoch 00469: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0038 - accuracy: 0.9962 - val_loss: 0.0240 - val_accuracy: 0.9738\n",
            "Epoch 470/500\n",
            "295/298 [============================>.] - ETA: 0s - loss: 0.0039 - accuracy: 0.9960\n",
            "Epoch 00470: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0041 - accuracy: 0.9958 - val_loss: 0.0299 - val_accuracy: 0.9683\n",
            "Epoch 471/500\n",
            "292/298 [============================>.] - ETA: 0s - loss: 0.0040 - accuracy: 0.9959\n",
            "Epoch 00471: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0040 - accuracy: 0.9959 - val_loss: 0.0246 - val_accuracy: 0.9746\n",
            "Epoch 472/500\n",
            "284/298 [===========================>..] - ETA: 0s - loss: 0.0036 - accuracy: 0.9964\n",
            "Epoch 00472: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0037 - accuracy: 0.9963 - val_loss: 0.0249 - val_accuracy: 0.9738\n",
            "Epoch 473/500\n",
            "290/298 [============================>.] - ETA: 0s - loss: 0.0038 - accuracy: 0.9962\n",
            "Epoch 00473: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0037 - accuracy: 0.9963 - val_loss: 0.0239 - val_accuracy: 0.9754\n",
            "Epoch 474/500\n",
            "288/298 [===========================>..] - ETA: 0s - loss: 0.0034 - accuracy: 0.9966\n",
            "Epoch 00474: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0037 - accuracy: 0.9963 - val_loss: 0.0239 - val_accuracy: 0.9754\n",
            "Epoch 475/500\n",
            "287/298 [===========================>..] - ETA: 0s - loss: 0.0038 - accuracy: 0.9962\n",
            "Epoch 00475: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0037 - accuracy: 0.9963 - val_loss: 0.0239 - val_accuracy: 0.9754\n",
            "Epoch 476/500\n",
            "280/298 [===========================>..] - ETA: 0s - loss: 0.0038 - accuracy: 0.9962\n",
            "Epoch 00476: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0037 - accuracy: 0.9963 - val_loss: 0.0238 - val_accuracy: 0.9754\n",
            "Epoch 477/500\n",
            "289/298 [============================>.] - ETA: 0s - loss: 0.0037 - accuracy: 0.9963\n",
            "Epoch 00477: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0037 - accuracy: 0.9963 - val_loss: 0.0238 - val_accuracy: 0.9754\n",
            "Epoch 478/500\n",
            "294/298 [============================>.] - ETA: 0s - loss: 0.0037 - accuracy: 0.9963\n",
            "Epoch 00478: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0037 - accuracy: 0.9963 - val_loss: 0.0238 - val_accuracy: 0.9754\n",
            "Epoch 479/500\n",
            "282/298 [===========================>..] - ETA: 0s - loss: 0.0036 - accuracy: 0.9963\n",
            "Epoch 00479: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0037 - accuracy: 0.9963 - val_loss: 0.0238 - val_accuracy: 0.9754\n",
            "Epoch 480/500\n",
            "290/298 [============================>.] - ETA: 0s - loss: 0.0036 - accuracy: 0.9963\n",
            "Epoch 00480: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0037 - accuracy: 0.9963 - val_loss: 0.0237 - val_accuracy: 0.9754\n",
            "Epoch 481/500\n",
            "286/298 [===========================>..] - ETA: 0s - loss: 0.0037 - accuracy: 0.9963\n",
            "Epoch 00481: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0037 - accuracy: 0.9963 - val_loss: 0.0237 - val_accuracy: 0.9754\n",
            "Epoch 482/500\n",
            "288/298 [===========================>..] - ETA: 0s - loss: 0.0037 - accuracy: 0.9963\n",
            "Epoch 00482: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0037 - accuracy: 0.9963 - val_loss: 0.0237 - val_accuracy: 0.9754\n",
            "Epoch 483/500\n",
            "296/298 [============================>.] - ETA: 0s - loss: 0.0037 - accuracy: 0.9963\n",
            "Epoch 00483: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0037 - accuracy: 0.9963 - val_loss: 0.0237 - val_accuracy: 0.9754\n",
            "Epoch 484/500\n",
            "290/298 [============================>.] - ETA: 0s - loss: 0.0036 - accuracy: 0.9963\n",
            "Epoch 00484: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0037 - accuracy: 0.9963 - val_loss: 0.0236 - val_accuracy: 0.9754\n",
            "Epoch 485/500\n",
            "278/298 [==========================>...] - ETA: 0s - loss: 0.0036 - accuracy: 0.9964\n",
            "Epoch 00485: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0037 - accuracy: 0.9963 - val_loss: 0.0237 - val_accuracy: 0.9754\n",
            "Epoch 486/500\n",
            "297/298 [============================>.] - ETA: 0s - loss: 0.0037 - accuracy: 0.9963\n",
            "Epoch 00486: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0037 - accuracy: 0.9963 - val_loss: 0.0236 - val_accuracy: 0.9754\n",
            "Epoch 487/500\n",
            "289/298 [============================>.] - ETA: 0s - loss: 0.0037 - accuracy: 0.9963\n",
            "Epoch 00487: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0037 - accuracy: 0.9963 - val_loss: 0.0237 - val_accuracy: 0.9754\n",
            "Epoch 488/500\n",
            "281/298 [===========================>..] - ETA: 0s - loss: 0.0035 - accuracy: 0.9964\n",
            "Epoch 00488: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0037 - accuracy: 0.9963 - val_loss: 0.0238 - val_accuracy: 0.9754\n",
            "Epoch 489/500\n",
            "282/298 [===========================>..] - ETA: 0s - loss: 0.0035 - accuracy: 0.9965\n",
            "Epoch 00489: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0037 - accuracy: 0.9963 - val_loss: 0.0239 - val_accuracy: 0.9746\n",
            "Epoch 490/500\n",
            "285/298 [===========================>..] - ETA: 0s - loss: 0.0038 - accuracy: 0.9962\n",
            "Epoch 00490: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0037 - accuracy: 0.9963 - val_loss: 0.0241 - val_accuracy: 0.9746\n",
            "Epoch 491/500\n",
            "278/298 [==========================>...] - ETA: 0s - loss: 0.0037 - accuracy: 0.9963\n",
            "Epoch 00491: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0037 - accuracy: 0.9963 - val_loss: 0.0251 - val_accuracy: 0.9738\n",
            "Epoch 492/500\n",
            "292/298 [============================>.] - ETA: 0s - loss: 0.0036 - accuracy: 0.9965\n",
            "Epoch 00492: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0036 - accuracy: 0.9964 - val_loss: 0.0281 - val_accuracy: 0.9698\n",
            "Epoch 493/500\n",
            "298/298 [==============================] - ETA: 0s - loss: 0.0038 - accuracy: 0.9962\n",
            "Epoch 00493: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0038 - accuracy: 0.9962 - val_loss: 0.0280 - val_accuracy: 0.9698\n",
            "Epoch 494/500\n",
            "293/298 [============================>.] - ETA: 0s - loss: 0.0035 - accuracy: 0.9965\n",
            "Epoch 00494: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0036 - accuracy: 0.9964 - val_loss: 0.0276 - val_accuracy: 0.9706\n",
            "Epoch 495/500\n",
            "283/298 [===========================>..] - ETA: 0s - loss: 0.0036 - accuracy: 0.9964\n",
            "Epoch 00495: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0036 - accuracy: 0.9964 - val_loss: 0.0276 - val_accuracy: 0.9706\n",
            "Epoch 496/500\n",
            "298/298 [==============================] - ETA: 0s - loss: 0.0036 - accuracy: 0.9964\n",
            "Epoch 00496: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0036 - accuracy: 0.9964 - val_loss: 0.0276 - val_accuracy: 0.9706\n",
            "Epoch 497/500\n",
            "282/298 [===========================>..] - ETA: 0s - loss: 0.0035 - accuracy: 0.9965\n",
            "Epoch 00497: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0036 - accuracy: 0.9964 - val_loss: 0.0276 - val_accuracy: 0.9706\n",
            "Epoch 498/500\n",
            "284/298 [===========================>..] - ETA: 0s - loss: 0.0036 - accuracy: 0.9964\n",
            "Epoch 00498: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0036 - accuracy: 0.9964 - val_loss: 0.0276 - val_accuracy: 0.9706\n",
            "Epoch 499/500\n",
            "284/298 [===========================>..] - ETA: 0s - loss: 0.0035 - accuracy: 0.9965\n",
            "Epoch 00499: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0036 - accuracy: 0.9964 - val_loss: 0.0276 - val_accuracy: 0.9706\n",
            "Epoch 500/500\n",
            "285/298 [===========================>..] - ETA: 0s - loss: 0.0036 - accuracy: 0.9964\n",
            "Epoch 00500: val_loss did not improve from 0.01578\n",
            "298/298 [==============================] - 1s 3ms/step - loss: 0.0036 - accuracy: 0.9964 - val_loss: 0.0275 - val_accuracy: 0.9706\n",
            "training time: 406.998 s\n",
            "predict time: 0.162 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lv6lKsneZEyd",
        "outputId": "be86e1a9-1bfa-487c-bdb3-636915399383"
      },
      "source": [
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_16 (Dense)            (None, 30)                1620      \n",
            "                                                                 \n",
            " dense_17 (Dense)            (None, 40)                1240      \n",
            "                                                                 \n",
            " dense_18 (Dense)            (None, 10)                410       \n",
            "                                                                 \n",
            " dense_19 (Dense)            (None, 1)                 11        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,281\n",
            "Trainable params: 3,281\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1oKp7o07IiEO",
        "outputId": "acf5caaf-4d6f-4b9c-a540-7113d68bc75b"
      },
      "source": [
        "model.evaluate(X_res,y_res)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "298/298 [==============================] - 0s 1ms/step - loss: 0.0036 - accuracy: 0.9964\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.0035553963389247656, 0.9964308142662048]"
            ]
          },
          "metadata": {},
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOWhRWrIZEvd",
        "outputId": "ecd93b4c-5fce-436b-8cd0-09a7cac2b23e"
      },
      "source": [
        "model.evaluate(X_test,y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "40/40 [==============================] - 0s 2ms/step - loss: 0.0275 - accuracy: 0.9706\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.02753525972366333, 0.970634937286377]"
            ]
          },
          "metadata": {},
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmMWn9fQZEst",
        "outputId": "d86d6ac5-43eb-42c7-8a44-6c8dd0953ee7"
      },
      "source": [
        "min(history.history['val_loss'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.015783248469233513"
            ]
          },
          "metadata": {},
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWWRpsxwZPde",
        "outputId": "8eb45d6a-03b8-4683-ba0c-67de392c7aa2"
      },
      "source": [
        "y_pred_final=list()\n",
        "for i in range(len(y_pred)):\n",
        "  if(y_pred[i]>0.5):\n",
        "    print('%.2f (expected %d)' % (1, y_test[i]))\n",
        "    y_pred_final.append(1)\n",
        "\n",
        "  else:\n",
        "        print('%.2f (expected %d)' % (0, y_test[i]))\n",
        "        y_pred_final.append(0)\n",
        "\n",
        "\n",
        "_, accuracy = model.evaluate(X_train,y_train.ravel())\n",
        "print('Training Accuracy: %.2f\\n\\n' % (accuracy*100))\n",
        "\n",
        "_, accuracy = model.evaluate(X_test,y_test)\n",
        "print('Testing Accuracy: %.2f\\n\\n' % (accuracy*100))\n",
        "print(y_pred_final)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "0.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "0.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "1.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "0.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "0.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "0.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "0.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "1.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "0.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "0.00 (expected 1)\n",
            "0.00 (expected 1)\n",
            "0.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "0.00 (expected 1)\n",
            "0.00 (expected 1)\n",
            "0.00 (expected 1)\n",
            "0.00 (expected 1)\n",
            "0.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "0.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "0.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "0.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "0.00 (expected 0)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "0.00 (expected 1)\n",
            "0.00 (expected 1)\n",
            "0.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "0.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "0.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "0.00 (expected 1)\n",
            "0.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "0.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "0.00 (expected 1)\n",
            "0.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "0.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "1.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "1.00 (expected 0)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "0.00 (expected 1)\n",
            "0.00 (expected 1)\n",
            "0.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "1.00 (expected 1)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "0.00 (expected 0)\n",
            "184/184 [==============================] - 1s 3ms/step - loss: 0.0029 - accuracy: 0.9971\n",
            "Training Accuracy: 99.71\n",
            "\n",
            "\n",
            "40/40 [==============================] - 0s 1ms/step - loss: 0.0275 - accuracy: 0.9706\n",
            "Testing Accuracy: 97.06\n",
            "\n",
            "\n",
            "[1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wtBjuHVzZPa1",
        "outputId": "58ddc0d6-2f72-4b9c-b68f-4d4c139b413b"
      },
      "source": [
        "from sklearn import metrics\n",
        "print('MAE:', metrics.mean_absolute_error(y_test, y_pred))\n",
        "print('MSE:', metrics.mean_squared_error(y_test, y_pred))\n",
        "print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
        "print('VarScore:',metrics.explained_variance_score(y_test,y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAE: 0.033245417002647645\n",
            "MSE: 0.027535258486937244\n",
            "RMSE: 0.16593751380244687\n",
            "VarScore: 0.7214335055597626\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKqrXDA5dMej",
        "outputId": "542fe521-2152-4f2b-d567-f336b6df8c05"
      },
      "source": [
        "score_ANN = accuracy_score(y_pred_final,y_test)*100\n",
        "print(\"The accuracy score achieved using ANN is: \"+str(score_ANN)+\" %\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The accuracy score achieved using ANN is: 97.06349206349206 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-TJm4p7aGO8",
        "outputId": "95986e09-7cc0-4ce6-c89a-7f92ede68217"
      },
      "source": [
        "#Precision,recall,f1_score,cohen_kappa_score,auc.......\n",
        "print(\"Precision,recall,f1 score,cohen kappa score,auc.....\")\n",
        "print(\" \")\n",
        "\n",
        "# precision tp / (tp + fp)\n",
        "precision = precision_score(y_test, y_pred_final)\n",
        "print('Precision: %f' % precision)\n",
        "# recall: tp / (tp + fn)\n",
        "recall = recall_score(y_test,y_pred_final)\n",
        "print('Recall: %f' % recall)\n",
        "# f1: 2 tp / (2 tp + fp + fn)\n",
        "f1 = f1_score(y_test, y_pred_final)\n",
        "print('F1 score: %f' % f1)\n",
        "\n",
        "# ROC AUC\n",
        "auc = roc_auc_score(y_test,  y_pred_final)\n",
        "print('ROC AUC: %f' % auc)\n",
        "#Cohen's kappa\n",
        "kappa=cohen_kappa_score(y_test,  y_pred_final)\n",
        "print('Cohen Kappa: %f' % kappa)\n",
        "print(\" \")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision,recall,f1 score,cohen kappa score,auc.....\n",
            " \n",
            "Precision: 0.996347\n",
            "Recall: 0.970641\n",
            "F1 score: 0.983326\n",
            "ROC AUC: 0.970614\n",
            "Cohen Kappa: 0.860578\n",
            " \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biGBZlw9juIh"
      },
      "source": [
        "#model = Sequential()\n",
        "#model.add(Dense(15,input_dim=53,activation='sigmoid'))\n",
        "#model.add(Dense(20,activation='sigmoid'))\n",
        "#model.add(Dense(10,activation='relu'))\n",
        "#model.add(Dense(1,activation='sigmoid'))\n",
        "\n",
        "#model.compile(optimizer='Adam',loss='binary_crossentropy',metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T65PRDUvNoCr"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}